{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXR8OR8wxqce"
      },
      "source": [
        "Import libraries, define the dataset and transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu8j-OxJxpso",
        "outputId": "ffa54dab-8172-4006-cbd3-45d20608900b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set up device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class ModularArithmeticDataset(Dataset):\n",
        "    def __init__(self, data, encoder, max_seq_length, padding_char):\n",
        "        self.data = data\n",
        "        self.encoder = encoder\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.pad_token_id = self.encoder(padding_char)[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        equation = item['input']\n",
        "        result = item['output']\n",
        "\n",
        "        equation_tokens = self.encoder(equation)\n",
        "\n",
        "        # Pad or truncate to max_seq_length\n",
        "        if len(equation_tokens) < self.max_seq_length:\n",
        "            equation_tokens += [self.pad_token_id] * (self.max_seq_length - len(equation_tokens))\n",
        "        else:\n",
        "            equation_tokens = equation_tokens[:self.max_seq_length]\n",
        "\n",
        "        return torch.tensor(equation_tokens, dtype=torch.long), torch.tensor(int(result), dtype=torch.long)\n",
        "\n",
        "class ArithmeticTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, max_seq_length, max_result):\n",
        "        super(ArithmeticTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = nn.Embedding(max_seq_length, d_model)\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "\n",
        "        self.fc = nn.Linear(d_model, max_result)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src shape: [batch_size, seq_len]\n",
        "\n",
        "        # Create a mask for padded elements\n",
        "        mask = (src == 0).to(device)\n",
        "\n",
        "        # Create positional encodings\n",
        "        positions = torch.arange(0, src.size(1)).unsqueeze(0).expand(src.size()).to(device)\n",
        "\n",
        "        # Combine token embeddings and positional encodings\n",
        "        x = self.embedding(src) + self.pos_encoder(positions)\n",
        "\n",
        "        # Pass through the transformer\n",
        "        output = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
        "\n",
        "        # Use the output of the last non-padded token for classification\n",
        "        output = output[:, 0, :]  # Use the first token's output for classification\n",
        "\n",
        "        # Project to the number of possible results\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "def training_loop(num_epochs, optimizer, train_loader, test_loader, model, criterion):\n",
        "  # Training loop\n",
        "  start_time = datetime.now()\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      total_loss = 0\n",
        "      for equation, result in train_loader:\n",
        "          equation, result = equation.to(device), result.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          output = model(equation)\n",
        "          loss = criterion(output, result)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      # Validation\n",
        "      model.eval()\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      with torch.no_grad():\n",
        "          for equation, result in test_loader:\n",
        "              equation, result = equation.to(device), result.to(device)\n",
        "              output = model(equation)\n",
        "              _, predicted = torch.max(output, 1)\n",
        "              total += result.size(0)\n",
        "              correct += (predicted == result).sum().item()\n",
        "\n",
        "      accuracy = 100 * correct / total\n",
        "      print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader):.3f}, Accuracy: {accuracy:.2f}%\")\n",
        "  end_time = datetime.now()\n",
        "  training_duration = end_time - start_time\n",
        "  print(f\"Training duration: {training_duration.total_seconds()} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81IBskIR1lw6"
      },
      "source": [
        "Import the data and create the tokenizer functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izTfGIPx1zr8",
        "outputId": "59f9ecfd-d626-4093-9bef-e06345129004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 8000\n",
            "Test dataset size: 2000\n",
            "Train data sample:\n",
            "                 input  output  modulus       operation\n",
            "0   (70 * 66 * 2) % 10       0       10  multiplication\n",
            "1  (80 + 33 + 79) % 10       2       10        addition\n",
            "2  (18 + 34 * 74) % 10       4       10     addAndMulti\n",
            "3  (56 + 62 + 98) % 10       6       10        addition\n",
            "4  (73 * 73 + 78) % 10       7       10     addAndMulti\n",
            "vocab_size = 17\n",
            "encode(%6+370*89)P42(51 ) = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
            "decode(encode((70 * 66 * 2) % 10)) = (70 * 66 * 2) % 10\n"
          ]
        }
      ],
      "source": [
        "# Get paths to the test and train data\n",
        "data_set_name = \"modular_arithmetic_three_numbers\"\n",
        "data_path = os.path.join(\"..\", \"data\")\n",
        "train_data_path = os.path.join(data_path,f'{data_set_name}_train.csv')\n",
        "test_data_path = os.path.join(data_path, f'{data_set_name}_test.csv')\n",
        "\n",
        "# Load and preprocess the data\n",
        "train_data = pd.read_csv(train_data_path)\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "print(\"Train dataset size:\", len(train_data))\n",
        "print(\"Test dataset size:\", len(test_data))\n",
        "\n",
        "print(\"Train data sample:\")\n",
        "print(train_data.head())\n",
        "\n",
        "# Get set of unique characters in training data input\n",
        "unique_characters = set(\"\".join(train_data['input']))\n",
        "padding_char = \"P\"\n",
        "unique_characters.add(padding_char) # Add character for padding\n",
        "charToInt = {char: i for i, char in enumerate(unique_characters)}\n",
        "intToChar = {i: char for i, char in enumerate(unique_characters)}\n",
        "vocab_size = len(unique_characters)\n",
        "\n",
        "encoder = lambda string: [charToInt[char] for char in string]\n",
        "decoder = lambda arr: \"\".join([intToChar[i] for i in arr])\n",
        "\n",
        "all_chars = \"\".join(unique_characters)\n",
        "print(f\"vocab_size = {vocab_size}\")\n",
        "print(f'encode({all_chars}) = {encoder(all_chars)}')\n",
        "print(f'decode(encode({train_data[\"input\"][0]})) = {decoder(encoder(train_data[\"input\"][0]))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT1CHI992IMe"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bvMMr1_2O16",
        "outputId": "c7b16484-3907-49ea-a370-f1389acb732f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/80, Loss: 2.160, Accuracy: 21.75%\n",
            "Epoch 2/80, Loss: 1.985, Accuracy: 25.30%\n",
            "Epoch 3/80, Loss: 1.840, Accuracy: 29.15%\n",
            "Epoch 4/80, Loss: 1.706, Accuracy: 31.55%\n",
            "Epoch 5/80, Loss: 1.634, Accuracy: 33.05%\n",
            "Epoch 6/80, Loss: 1.581, Accuracy: 35.50%\n",
            "Epoch 7/80, Loss: 1.542, Accuracy: 34.85%\n",
            "Epoch 8/80, Loss: 1.510, Accuracy: 36.40%\n",
            "Epoch 9/80, Loss: 1.457, Accuracy: 40.15%\n",
            "Epoch 10/80, Loss: 1.391, Accuracy: 41.00%\n",
            "Epoch 11/80, Loss: 1.356, Accuracy: 42.30%\n",
            "Epoch 12/80, Loss: 1.308, Accuracy: 44.50%\n",
            "Epoch 13/80, Loss: 1.273, Accuracy: 46.35%\n",
            "Epoch 14/80, Loss: 1.233, Accuracy: 47.85%\n",
            "Epoch 15/80, Loss: 1.206, Accuracy: 46.70%\n",
            "Epoch 16/80, Loss: 1.193, Accuracy: 47.05%\n",
            "Epoch 17/80, Loss: 1.154, Accuracy: 47.95%\n",
            "Epoch 18/80, Loss: 1.157, Accuracy: 49.75%\n",
            "Epoch 19/80, Loss: 1.144, Accuracy: 47.90%\n",
            "Epoch 20/80, Loss: 1.133, Accuracy: 49.60%\n",
            "Epoch 21/80, Loss: 1.120, Accuracy: 50.50%\n",
            "Epoch 22/80, Loss: 1.109, Accuracy: 48.40%\n",
            "Epoch 23/80, Loss: 1.100, Accuracy: 51.50%\n",
            "Epoch 24/80, Loss: 1.081, Accuracy: 52.60%\n",
            "Epoch 25/80, Loss: 1.068, Accuracy: 52.45%\n",
            "Epoch 26/80, Loss: 1.042, Accuracy: 55.55%\n",
            "Epoch 27/80, Loss: 1.036, Accuracy: 57.10%\n",
            "Epoch 28/80, Loss: 1.001, Accuracy: 61.20%\n",
            "Epoch 29/80, Loss: 0.949, Accuracy: 65.75%\n",
            "Epoch 30/80, Loss: 0.922, Accuracy: 66.25%\n",
            "Epoch 31/80, Loss: 0.890, Accuracy: 67.95%\n",
            "Epoch 32/80, Loss: 0.839, Accuracy: 72.45%\n",
            "Epoch 33/80, Loss: 0.810, Accuracy: 72.95%\n",
            "Epoch 34/80, Loss: 0.765, Accuracy: 75.70%\n",
            "Epoch 35/80, Loss: 0.719, Accuracy: 77.55%\n",
            "Epoch 36/80, Loss: 0.681, Accuracy: 80.10%\n",
            "Epoch 37/80, Loss: 0.641, Accuracy: 84.30%\n",
            "Epoch 38/80, Loss: 0.601, Accuracy: 85.75%\n",
            "Epoch 39/80, Loss: 0.580, Accuracy: 87.45%\n",
            "Epoch 40/80, Loss: 0.515, Accuracy: 89.15%\n",
            "Epoch 41/80, Loss: 0.507, Accuracy: 91.55%\n",
            "Epoch 42/80, Loss: 0.458, Accuracy: 94.95%\n",
            "Epoch 43/80, Loss: 0.411, Accuracy: 94.05%\n",
            "Epoch 44/80, Loss: 0.396, Accuracy: 95.50%\n",
            "Epoch 45/80, Loss: 0.358, Accuracy: 96.45%\n",
            "Epoch 46/80, Loss: 0.356, Accuracy: 96.05%\n",
            "Epoch 47/80, Loss: 0.312, Accuracy: 96.25%\n",
            "Epoch 48/80, Loss: 0.285, Accuracy: 97.45%\n",
            "Epoch 49/80, Loss: 0.267, Accuracy: 98.55%\n",
            "Epoch 50/80, Loss: 0.260, Accuracy: 98.50%\n",
            "Epoch 51/80, Loss: 0.246, Accuracy: 98.20%\n",
            "Epoch 52/80, Loss: 0.236, Accuracy: 98.75%\n",
            "Epoch 53/80, Loss: 0.210, Accuracy: 98.75%\n",
            "Epoch 54/80, Loss: 0.225, Accuracy: 98.55%\n",
            "Epoch 55/80, Loss: 0.216, Accuracy: 98.55%\n",
            "Epoch 56/80, Loss: 0.184, Accuracy: 99.10%\n",
            "Epoch 57/80, Loss: 0.188, Accuracy: 99.10%\n",
            "Epoch 58/80, Loss: 0.171, Accuracy: 99.15%\n",
            "Epoch 59/80, Loss: 0.171, Accuracy: 99.10%\n",
            "Epoch 60/80, Loss: 0.184, Accuracy: 99.45%\n",
            "Epoch 61/80, Loss: 0.180, Accuracy: 99.40%\n",
            "Epoch 62/80, Loss: 0.165, Accuracy: 99.50%\n",
            "Epoch 63/80, Loss: 0.162, Accuracy: 99.70%\n",
            "Epoch 64/80, Loss: 0.141, Accuracy: 99.65%\n",
            "Epoch 65/80, Loss: 0.158, Accuracy: 99.45%\n",
            "Epoch 66/80, Loss: 0.152, Accuracy: 99.50%\n",
            "Epoch 67/80, Loss: 0.138, Accuracy: 99.45%\n",
            "Epoch 68/80, Loss: 0.149, Accuracy: 99.25%\n",
            "Epoch 69/80, Loss: 0.134, Accuracy: 99.35%\n",
            "Epoch 70/80, Loss: 0.152, Accuracy: 99.50%\n",
            "Epoch 71/80, Loss: 0.132, Accuracy: 99.55%\n",
            "Epoch 72/80, Loss: 0.124, Accuracy: 99.55%\n",
            "Epoch 73/80, Loss: 0.117, Accuracy: 99.40%\n",
            "Epoch 74/80, Loss: 0.155, Accuracy: 99.70%\n",
            "Epoch 75/80, Loss: 0.140, Accuracy: 99.40%\n",
            "Epoch 76/80, Loss: 0.120, Accuracy: 99.20%\n",
            "Epoch 77/80, Loss: 0.103, Accuracy: 99.60%\n",
            "Epoch 78/80, Loss: 0.108, Accuracy: 99.50%\n",
            "Epoch 79/80, Loss: 0.110, Accuracy: 99.65%\n",
            "Epoch 80/80, Loss: 0.097, Accuracy: 99.60%\n",
            "Training duration: 155.04956 seconds\n"
          ]
        }
      ],
      "source": [
        "# Determine max_length based on the longest equation in the dataset\n",
        "max_seq_length = train_data['input'].str.len().max()\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = ModularArithmeticDataset(train_data, encoder, max_seq_length, padding_char)\n",
        "test_dataset = ModularArithmeticDataset(test_data, encoder, max_seq_length, padding_char)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Model parameters\n",
        "d_model = 32\n",
        "nhead = 2\n",
        "num_layers = 3\n",
        "dim_feedforward = 128\n",
        "max_result = train_data['output'].max() + 1  # +1 because we start counting from 0\n",
        "\n",
        "# Initialize the model\n",
        "model = ArithmeticTransformer(vocab_size, d_model, nhead, num_layers, dim_feedforward, max_seq_length, max_result)\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), weight_decay=1e-5)\n",
        "\n",
        "num_epochs =80\n",
        "\n",
        "training_loop(num_epochs=num_epochs,\n",
        "              optimizer=optimizer,\n",
        "              train_loader=train_loader,\n",
        "              test_loader=test_loader,\n",
        "              model=model,\n",
        "              criterion=criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rsogUIV5TIH"
      },
      "source": [
        "Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_MJHsjs5SYb",
        "outputId": "6ee5222f-64d3-4d08-ad8c-f8c99cd227f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Equation: (30 * 42 + 13) % 10, Predicted: 3, Actual: 3\n",
            "Equation: (85 + 7 * 61) % 10, Predicted: 2, Actual: 2\n",
            "Equation: (55 * 40 * 3) % 10, Predicted: 0, Actual: 0\n",
            "Equation: (60 + 42 + 91) % 10, Predicted: 3, Actual: 3\n",
            "Equation: (16 + 58 + 0) % 10, Predicted: 4, Actual: 4\n",
            "Equation: (96 + 92 + 16) % 10, Predicted: 4, Actual: 4\n",
            "Equation: (5 * 70 * 45) % 10, Predicted: 0, Actual: 0\n",
            "Equation: (5 * 5 + 25) % 10, Predicted: 0, Actual: 0\n",
            "Equation: (21 * 1 * 44) % 10, Predicted: 4, Actual: 4\n",
            "Equation: (35 + 29 + 96) % 10, Predicted: 0, Actual: 0\n",
            "Equation: (0 * 36 * 18) % 10, Predicted: 0, Actual: 0\n",
            "Equation: (92 * 19 + 63) % 10, Predicted: 1, Actual: 1\n",
            "Equation: (41 * 33 + 21) % 10, Predicted: 4, Actual: 4\n",
            "Equation: (7 + 59 * 12) % 10, Predicted: 5, Actual: 5\n",
            "Equation: (42 * 82 + 29) % 10, Predicted: 3, Actual: 3\n",
            "Equation: (33 * 70 + 7) % 10, Predicted: 7, Actual: 7\n",
            "Equation: (20 + 17 * 31) % 10, Predicted: 7, Actual: 7\n",
            "Equation: (82 * 32 + 76) % 10, Predicted: 0, Actual: 0\n",
            "Equation: (49 + 13 + 10) % 10, Predicted: 2, Actual: 2\n",
            "Equation: (46 + 17 * 87) % 10, Predicted: 5, Actual: 5\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_samples = 20  # Number of samples to test\n",
        "    sample_count = 0\n",
        "    for equation, result in test_loader:\n",
        "        equation, result = equation.to(device), result.to(device)\n",
        "        output = model(equation)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        for i in range(len(equation)):\n",
        "            if sample_count >= test_samples:\n",
        "                break\n",
        "            eq_str = decoder(equation[i].tolist())\n",
        "            eq_str = eq_str.replace(padding_char, \"\")\n",
        "            wrong_message = \" <--- WRONG!\" if predicted[i] != result[i] else \"\"\n",
        "            print(f\"Equation: {eq_str}, Predicted: {predicted[i].item()}, Actual: {result[i].item()}{wrong_message}\")\n",
        "            sample_count += 1\n",
        "        if sample_count >= test_samples:\n",
        "            break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
