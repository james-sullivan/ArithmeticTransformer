{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_frac = 0.5 addition_frac = 0.7\n",
      "len(train_losses) = 6000 len(test_losses) = 6000 len(model_checkpoints) = 120\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "from neel_plotly.plot import line\n",
    "from helpers import loss_fn\n",
    "from devinterp.slt.sampler import estimate_learning_coeff_with_summary, estimate_learning_coeff, SGLD\n",
    "from devinterp.utils import plot_trace\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RAND_SEED = 42\n",
    "\n",
    "LOAD_LOCATION = \"../saves/check_point_50/grokking_add_multi_0.7.pth\"\n",
    "\n",
    "cached_data = torch.load(LOAD_LOCATION, weights_only=False)\n",
    "\n",
    "state_dict = cached_data['model']\n",
    "model_checkpoints = cached_data[\"checkpoints\"]\n",
    "checkpoint_epochs = cached_data[\"checkpoint_epochs\"]\n",
    "test_losses = cached_data['test_losses']\n",
    "train_losses = cached_data['train_losses']\n",
    "add_test_losses = cached_data['add_test_losses']\n",
    "multi_test_losses = cached_data['multi_test_losses']\n",
    "max_nums = cached_data['max_nums']\n",
    "mod_value = cached_data['mod_value']\n",
    "train_frac = cached_data['train_frac']\n",
    "addition_frac = cached_data['addition_frac']\n",
    "train_data = cached_data['train_data']\n",
    "\n",
    "print(f\"train_frac = {train_frac} addition_frac = {addition_frac}\")\n",
    "print(f\"len(train_losses) = {len(train_losses)} len(test_losses) = {len(test_losses)} len(model_checkpoints) = {len(model_checkpoints)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_dataloader' from 'helpers' (/Users/james/dev/ArithmeticTransformer/src/helpers.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_dataloader\n\u001b[1;32m      3\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m get_dataloader(train_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_dataloader' from 'helpers' (/Users/james/dev/ArithmeticTransformer/src/helpers.py)"
     ]
    }
   ],
   "source": [
    "from helpers import get_dataloader\n",
    "\n",
    "train_loader = get_dataloader(train_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = 1,\n",
    "    n_heads = 4,\n",
    "    d_model = 128,\n",
    "    d_head = 32,\n",
    "    d_mlp = 512,\n",
    "    act_fn = \"relu\",\n",
    "    normalization_type=\"LN\",\n",
    "    d_vocab=max_nums+1,\n",
    "    d_vocab_out=mod_value,\n",
    "    n_ctx= 3,\n",
    "    init_weights=True,\n",
    "    device=device,\n",
    "    seed = 999,\n",
    ")\n",
    "\n",
    "model = HookedTransformer(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Learning Coefficient (RLCT) Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "localization = 100.0\n",
    "num_chains = 3\n",
    "num_draws = 1000\n",
    "num_burnin_steps = 0\n",
    "num_steps_bw_draws = 1\n",
    "\n",
    "def evaluate(model, data):\n",
    "    inputs, outputs = data\n",
    "\n",
    "    return loss_fn(model(inputs), outputs), {\n",
    "        \"logits\": model(inputs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len(model_checkpoints) = {len(model_checkpoints)}\")\n",
    "\n",
    "model.load_state_dict(model_checkpoints[10])\n",
    "\n",
    "results = estimate_learning_coeff_with_summary(\n",
    "                model,\n",
    "                loader=train_loader,\n",
    "                evaluate=evaluate,\n",
    "                sampling_method=SGLD,\n",
    "                optimizer_kwargs=dict(lr=lr, localization=localization),\n",
    "                num_chains=num_chains,                  # How many independent chains to run\n",
    "                num_draws=num_draws,                    # How many samples to draw per chain\n",
    "                num_burnin_steps=num_burnin_steps,      # How many samples to discard at the beginning of each chain\n",
    "                num_steps_bw_draws=num_steps_bw_draws,  # How many steps to take between each sample\n",
    "                device=device,\n",
    "                online=True,\n",
    "            )\n",
    "\n",
    "plot_trace(\n",
    "    results[\"llc/trace\"],\n",
    "    \"Loss\",\n",
    "    x_axis=\"Step\",\n",
    "    title=f\"Loss Trace, avg LLC = {sum(results['llc/means']) / len(results['llc/means']):.2f}\",\n",
    "    plot_mean=False,\n",
    "    plot_std=False,\n",
    "    fig_size=(12, 9),\n",
    "    true_lc=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devinterp.slt.sampler import estimate_learning_coeff_with_summary, estimate_learning_coeff, SGLD\n",
    "from devinterp.utils import plot_trace\n",
    "\n",
    "def evaluate(model, data):\n",
    "    inputs, outputs = data\n",
    "\n",
    "    return loss_fn(model(inputs), outputs), {\n",
    "        \"logits\": model(inputs)\n",
    "    }\n",
    "\n",
    "llc_estimates = []\n",
    "\n",
    "for saved_model in tqdm.tqdm(model_checkpoints):\n",
    "    model.load_state_dict(saved_model)\n",
    "    result = estimate_learning_coeff(\n",
    "                model,\n",
    "                loader=train_loader,\n",
    "                evaluate=evaluate,\n",
    "                sampling_method=SGLD,\n",
    "                optimizer_kwargs=dict(lr=5e-4, localization=100.0),\n",
    "                num_chains=15,           # How many independent chains to run\n",
    "                num_draws=5,            # How many samples to draw per chain\n",
    "                num_burnin_steps=20,    # How many samples to discard at the beginning of each chain\n",
    "                num_steps_bw_draws=1,   # How many steps to take between each sample\n",
    "                device=device,\n",
    "                #online=True,\n",
    "            )\n",
    "    llc_estimates.append(result)\n",
    "    print(f\"result = {result}\")\n",
    "\n",
    "# Graph the LLC estimates\n",
    "line(llc_estimates, xaxis=\"Epoch\", yaxis=\"LLC\", title=\"Learning Coefficient Estimates\", log_y=False, toggle_x=True, toggle_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_LOCATION = \"../saves/check_point_50/llc_estimates.pth\"\n",
    "SAVE = False\n",
    "if SAVE:\n",
    "    torch.save({\n",
    "        \"llc_estimates\": llc_estimates,\n",
    "    }, SAVE_LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some cool graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(llc_estimates, xaxis=\"Epoch\", yaxis=\"LLC\", title=\"Learning Coefficient Estimates\", log_y=False, toggle_x=True, toggle_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have your data prepared as before\n",
    "epochs = np.arange(0, 4000, 10)\n",
    "lc_epochs = np.arange(0, 4000, 100)\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# Create the main axis\n",
    "ax1 = plt.gca()\n",
    "\n",
    "# Plot loss curves\n",
    "ax1.plot(epochs, test_losses_avg[::10], color=colors[0], label='Test Loss', linewidth=2)\n",
    "ax1.plot(epochs, train_losses_avg[::10], color=colors[1], label='Train Loss', linewidth=2)\n",
    "#ax1.plot(epochs, add_test_losses_avg, color=colors[2], label='Addition Only Test Loss', linewidth=2)\n",
    "#ax1.plot(epochs, multi_test_losses_avg, color=colors[3], label='Multiplication Only Test Loss', linewidth=2)\n",
    "\n",
    "# Set up the second y-axis for learning coefficient\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(lc_epochs, llc_estimates[:4000], color=colors[4], linestyle='--', label='Learning Coefficient', linewidth=2)\n",
    "\n",
    "# Customize the plot\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Rolling Avg Loss', fontsize=12)\n",
    "ax2.set_ylabel('Learning Coefficient', fontsize=12)\n",
    "plt.title(f\"Loss and LLC During Training - {addition_frac*100:.0f}% Addition\", fontsize=14)\n",
    "\n",
    "# Adjust tick parameters\n",
    "ax1.tick_params(axis='both', which='major', labelsize=10)\n",
    "ax2.tick_params(axis='y', which='major', labelsize=10)\n",
    "\n",
    "# Create a combined legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right') # bbox_to_anchor=(1.1, 1), fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
