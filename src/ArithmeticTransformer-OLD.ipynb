{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXR8OR8wxqce"
      },
      "source": [
        "Import libraries, define the dataset and transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu8j-OxJxpso",
        "outputId": "ffa54dab-8172-4006-cbd3-45d20608900b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from devinterp.slt.sampler import estimate_learning_coeff_with_summary, SGLD\n",
        "from devinterp.utils import default_nbeta\n",
        "\n",
        "# Set up device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class ModularArithmeticDataset(Dataset):\n",
        "    def __init__(self, data, encoder, max_seq_length, padding_char):\n",
        "        self.data = data\n",
        "        self.encoder = encoder\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.pad_token_id = self.encoder(padding_char)[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        equation = item['input']\n",
        "        result = item['output']\n",
        "\n",
        "        equation_tokens = self.encoder(equation)\n",
        "\n",
        "        # Pad or truncate to max_seq_length\n",
        "        if len(equation_tokens) < self.max_seq_length:\n",
        "            equation_tokens += [self.pad_token_id] * (self.max_seq_length - len(equation_tokens))\n",
        "        else:\n",
        "            equation_tokens = equation_tokens[:self.max_seq_length]\n",
        "\n",
        "        return torch.tensor(equation_tokens, dtype=torch.long), torch.tensor(int(result), dtype=torch.long)\n",
        "\n",
        "class ArithmeticTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_layers, dim_feedforward, max_seq_length, max_result):\n",
        "        super(ArithmeticTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = nn.Embedding(max_seq_length, d_model)\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "\n",
        "        self.fc = nn.Linear(d_model, max_result)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src shape: [batch_size, seq_len]\n",
        "\n",
        "        # Create a mask for padded elements\n",
        "        mask = (src == 0).to(device)\n",
        "\n",
        "        # Create positional encodings\n",
        "        positions = torch.arange(0, src.size(1)).unsqueeze(0).expand(src.size()).to(device)\n",
        "\n",
        "        # Combine token embeddings and positional encodings\n",
        "        x = self.embedding(src) + self.pos_encoder(positions)\n",
        "\n",
        "        # Pass through the transformer\n",
        "        output = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
        "\n",
        "        # Use the output of the last non-padded token for classification\n",
        "        output = output[:, 0, :]  # Use the first token's output for classification\n",
        "\n",
        "        # Project to the number of possible results\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "def evaluate(model, data):\n",
        "    inputs, outputs = data\n",
        "\n",
        "    return nn.functional.cross_entropy(model(inputs).logits, outputs), {\n",
        "        \"logits\": model(inputs).logits\n",
        "    }  # transformers don't output a vector\n",
        "\n",
        "def training_loop(num_epochs, optimizer, train_loader, test_loader, model, criterion):\n",
        "\n",
        "    # Training loop\n",
        "    start_time = datetime.now()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for equation, result in train_loader:\n",
        "            equation, result = equation.to(device), result.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(equation)\n",
        "            loss = criterion(output, result)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for equation, result in test_loader:\n",
        "                equation, result = equation.to(device), result.to(device)\n",
        "                output = model(equation)\n",
        "                _, predicted = torch.max(output, 1)\n",
        "                total += result.size(0)\n",
        "                correct += (predicted == result).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        learning_coeff_stats = estimate_learning_coeff_with_summary(\n",
        "                                model,\n",
        "                                loader=train_loader,\n",
        "                                evaluate=criterion,\n",
        "                                sampling_method=SGLD,\n",
        "                                optimizer_kwargs=dict(lr=4e-4, localization=100.0),\n",
        "                                num_chains=3,  # How many independent chains to run\n",
        "                                num_draws=100,  # How many samples to draw per chain\n",
        "                                num_burnin_steps=0,  # How many samples to discard at the beginning of each chain\n",
        "                                num_steps_bw_draws=1,  # How many steps to take between each sample\n",
        "                                device=device,\n",
        "                                online=True)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader):.3f}, Accuracy: {accuracy:.2f}% LLC: {llc_mean:.3f}\")\n",
        "        \n",
        "    end_time = datetime.now()\n",
        "    training_duration = end_time - start_time\n",
        "    print(f\"Training duration: {training_duration.total_seconds():0.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81IBskIR1lw6"
      },
      "source": [
        "### Import the data and create the tokenizer functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izTfGIPx1zr8",
        "outputId": "59f9ecfd-d626-4093-9bef-e06345129004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 8000\n",
            "Test dataset size: 2000\n",
            "Train data sample:\n",
            "                 input  output  modulus       operation\n",
            "0   (70 * 66 * 2) % 10       0       10  multiplication\n",
            "1  (80 + 33 + 79) % 10       2       10        addition\n",
            "2  (18 + 34 * 74) % 10       4       10     addAndMulti\n",
            "3  (56 + 62 + 98) % 10       6       10        addition\n",
            "4  (73 * 73 + 78) % 10       7       10     addAndMulti\n",
            "vocab_size = 17\n",
            "encode(%6+370*89)P42(51 ) = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
            "decode(encode((70 * 66 * 2) % 10)) = (70 * 66 * 2) % 10\n"
          ]
        }
      ],
      "source": [
        "# Get paths to the test and train data\n",
        "data_set_name = \"modular_arithmetic_three_numbers\"\n",
        "data_path = os.path.join(\"..\", \"data\")\n",
        "train_data_path = os.path.join(data_path,f'{data_set_name}_train.csv')\n",
        "test_data_path = os.path.join(data_path, f'{data_set_name}_test.csv')\n",
        "\n",
        "# Load and preprocess the data\n",
        "train_data = pd.read_csv(train_data_path)\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "\n",
        "print(\"Train dataset size:\", len(train_data))\n",
        "print(\"Test dataset size:\", len(test_data))\n",
        "\n",
        "print(\"Train data sample:\")\n",
        "print(train_data.head())\n",
        "\n",
        "# Get set of unique characters in training data input\n",
        "unique_characters = set(\"\".join(train_data['input']))\n",
        "padding_char = \"P\"\n",
        "unique_characters.add(padding_char) # Add character for padding\n",
        "charToInt = {char: i for i, char in enumerate(unique_characters)}\n",
        "intToChar = {i: char for i, char in enumerate(unique_characters)}\n",
        "vocab_size = len(unique_characters)\n",
        "\n",
        "encoder = lambda string: [charToInt[char] for char in string]\n",
        "decoder = lambda arr: \"\".join([intToChar[i] for i in arr])\n",
        "\n",
        "all_chars = \"\".join(unique_characters)\n",
        "print(f\"vocab_size = {vocab_size}\")\n",
        "print(f'encode({all_chars}) = {encoder(all_chars)}')\n",
        "print(f'decode(encode({train_data[\"input\"][0]})) = {decoder(encoder(train_data[\"input\"][0]))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT1CHI992IMe"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bvMMr1_2O16",
        "outputId": "c7b16484-3907-49ea-a370-f1389acb732f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/james/Library/CloudStorage/OneDrive-Personal/ProgrammingProjects/ArithmeticTransformer/.venv/lib/python3.12/site-packages/devinterp/slt/sampler.py:54: UserWarning: nbeta not set - using default nbeta.\n",
            "  warnings.warn(\"nbeta not set - using default nbeta.\")\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not ArithmeticTransformer",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[109], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m     25\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m              \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m              \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m              \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m              \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[107], line 114\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(num_epochs, optimizer, train_loader, test_loader, model, criterion)\u001b[0m\n\u001b[1;32m    110\u001b[0m             correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m result)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    112\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[0;32m--> 114\u001b[0m     learning_coeff_stats \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_learning_coeff_with_summary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m                            \u001b[49m\u001b[43msampling_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSGLD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m                            \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocalization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# How many independent chains to run\u001b[39;49;00m\n\u001b[1;32m    121\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mnum_draws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# How many samples to draw per chain\u001b[39;49;00m\n\u001b[1;32m    122\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mnum_burnin_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# How many samples to discard at the beginning of each chain\u001b[39;49;00m\n\u001b[1;32m    123\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mnum_steps_bw_draws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# How many steps to take between each sample\u001b[39;49;00m\n\u001b[1;32m    124\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m                            \u001b[49m\u001b[43monline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% LLC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllc_mean\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
            "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/ProgrammingProjects/ArithmeticTransformer/.venv/lib/python3.12/site-packages/devinterp/slt/sampler.py:58\u001b[0m, in \u001b[0;36mestimate_learning_coeff_with_summary\u001b[0;34m(model, loader, callbacks, evaluate, sampling_method, optimizer_kwargs, num_draws, num_chains, num_burnin_steps, num_steps_bw_draws, init_loss, grad_accum_steps, cores, seed, device, verbose, optimize_over_per_model_param, online)\u001b[0m\n\u001b[1;32m     56\u001b[0m optimizer_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnbeta\u001b[39m\u001b[38;5;124m\"\u001b[39m, default_nbeta(loader))\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m init_loss:\n\u001b[0;32m---> 58\u001b[0m     init_loss \u001b[38;5;241m=\u001b[39m \u001b[43mget_init_loss_multi_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# alternative: init_loss = get_init_loss_full_batch(loader, model, evaluate, device)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# alternative: init_loss = get_init_loss_one_batch(loader, model, evaluate, device)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m online:\n",
            "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/ProgrammingProjects/ArithmeticTransformer/.venv/lib/python3.12/site-packages/devinterp/utils.py:179\u001b[0m, in \u001b[0;36mget_init_loss_multi_batch\u001b[0;34m(dataloader, n_batches, model, evaluate, device)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m islice(dataloader, n_batches):\n\u001b[1;32m    178\u001b[0m         data \u001b[38;5;241m=\u001b[39m prepare_input(data, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 179\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m split_results(\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss \u001b[38;5;241m/\u001b[39m n_batches\n",
            "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/ProgrammingProjects/ArithmeticTransformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/ProgrammingProjects/ArithmeticTransformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/ProgrammingProjects/ArithmeticTransformer/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/ProgrammingProjects/ArithmeticTransformer/.venv/lib/python3.12/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not ArithmeticTransformer"
          ]
        }
      ],
      "source": [
        "# Determine max_length based on the longest equation in the dataset\n",
        "max_seq_length = train_data['input'].str.len().max()\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = ModularArithmeticDataset(train_data, encoder, max_seq_length, padding_char)\n",
        "test_dataset = ModularArithmeticDataset(test_data, encoder, max_seq_length, padding_char)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Model parameters\n",
        "d_model = 32\n",
        "nhead = 2\n",
        "num_layers = 3\n",
        "dim_feedforward = 128\n",
        "max_result = train_data['output'].max() + 1  # +1 because we start counting from 0\n",
        "\n",
        "# Initialize the model\n",
        "model = ArithmeticTransformer(vocab_size, d_model, nhead, num_layers, dim_feedforward, max_seq_length, max_result)\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), weight_decay=1e-5)\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "training_loop(num_epochs=num_epochs,\n",
        "              optimizer=optimizer,\n",
        "              train_loader=train_loader,\n",
        "              test_loader=test_loader,\n",
        "              model=model,\n",
        "              criterion=criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rsogUIV5TIH"
      },
      "source": [
        "Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_MJHsjs5SYb",
        "outputId": "6ee5222f-64d3-4d08-ad8c-f8c99cd227f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Equation: (30 * 42 + 13) % 10, Predicted: 3, Actual: 3\n",
            "Equation: (85 + 7 * 61) % 10, Predicted: 2, Actual: 2\n",
            "Equation: (55 * 40 * 3) % 10, Predicted: 0, Actual: 0\n",
            "Equation: (60 + 42 + 91) % 10, Predicted: 3, Actual: 3\n",
            "Equation: (16 + 58 + 0) % 10, Predicted: 4, Actual: 4\n",
            "Equation: (96 + 92 + 16) % 10, Predicted: 4, Actual: 4\n",
            "Equation: (5 * 70 * 45) % 10, Predicted: 0, Actual: 0\n",
            "Equation: (5 * 5 + 25) % 10, Predicted: 0, Actual: 0\n",
            "Equation: (21 * 1 * 44) % 10, Predicted: 4, Actual: 4\n",
            "Equation: (35 + 29 + 96) % 10, Predicted: 0, Actual: 0\n",
            "Equation: (0 * 36 * 18) % 10, Predicted: 0, Actual: 0\n",
            "Equation: (92 * 19 + 63) % 10, Predicted: 1, Actual: 1\n",
            "Equation: (41 * 33 + 21) % 10, Predicted: 4, Actual: 4\n",
            "Equation: (7 + 59 * 12) % 10, Predicted: 5, Actual: 5\n",
            "Equation: (42 * 82 + 29) % 10, Predicted: 3, Actual: 3\n",
            "Equation: (33 * 70 + 7) % 10, Predicted: 7, Actual: 7\n",
            "Equation: (20 + 17 * 31) % 10, Predicted: 7, Actual: 7\n",
            "Equation: (82 * 32 + 76) % 10, Predicted: 0, Actual: 0\n",
            "Equation: (49 + 13 + 10) % 10, Predicted: 2, Actual: 2\n",
            "Equation: (46 + 17 * 87) % 10, Predicted: 5, Actual: 5\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_samples = 20  # Number of samples to test\n",
        "    sample_count = 0\n",
        "    for equation, result in test_loader:\n",
        "        equation, result = equation.to(device), result.to(device)\n",
        "        output = model(equation)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        for i in range(len(equation)):\n",
        "            if sample_count >= test_samples:\n",
        "                break\n",
        "            eq_str = decoder(equation[i].tolist())\n",
        "            eq_str = eq_str.replace(padding_char, \"\")\n",
        "            wrong_message = \" <--- WRONG!\" if predicted[i] != result[i] else \"\"\n",
        "            print(f\"Equation: {eq_str}, Predicted: {predicted[i].item()}, Actual: {result[i].item()}{wrong_message}\")\n",
        "            sample_count += 1\n",
        "        if sample_count >= test_samples:\n",
        "            break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
