{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A9Wv1do2UV2"
      },
      "source": [
        "# Dev Interp - Grokking Modular Addition and Multiplication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A13-7Mc62UV3"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Q998xCML2UV3"
      },
      "outputs": [],
      "source": [
        "TRAIN_MODEL = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "zUBTNnz62UV3",
        "outputId": "3658e07b-1cba-42b8-f23b-a319f209561a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running as a Jupyter notebook - intended for development only!\n",
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/85/m4tlmmlj4w58khykxvwgvt980000gn/T/ipykernel_13224/2858945602.py:22: DeprecationWarning:\n",
            "\n",
            "`magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "\n",
            "/var/folders/85/m4tlmmlj4w58khykxvwgvt980000gn/T/ipykernel_13224/2858945602.py:23: DeprecationWarning:\n",
            "\n",
            "`magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "import os\n",
        "\n",
        "DEVELOPMENT_MODE = True\n",
        "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "\n",
        "    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working\n",
        "    # # Install another version of node that makes PySvelte work way faster\n",
        "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
        "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")\n",
        "\n",
        "if IN_COLAB or IN_GITHUB:\n",
        "    %pip install transformer_lens\n",
        "    %pip install circuitsvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "I8nEQDlP2UV4",
        "outputId": "a46ab340-d447-407c-9e3d-36be3ef04ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using renderer: notebook_connected\n"
          ]
        }
      ],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import plotly.io as pio\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "rp4RGtCp2UV4"
      },
      "outputs": [],
      "source": [
        "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.title.font.size = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "RpPUz36r2UV4"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import os\n",
        "import tqdm.auto as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from typing import List, Union, Optional\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets\n",
        "from IPython.display import HTML\n",
        "\n",
        "from helpers import rolling_average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "NmIM5yWr2UV4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "import transformer_lens.utils as utils\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5bjmWrq2UV5"
      },
      "source": [
        "Plotting helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "aZe03f--2UV5"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EE1YCCg2UV5"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "2-6AgR5N2UV5"
      },
      "outputs": [],
      "source": [
        "DATA_SEED = 598"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqhvJcjT2UV5"
      },
      "source": [
        "## Create the dataset\n",
        "* Define modular addition and modular multiplication datasets\n",
        "* Combine the two data sets\n",
        "* We can vary the proportion of addition to multiplication in the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Addition dataset size = 16900\n",
            "Multiplication dataset size = 16900\n",
            "Total train size = 16900\n",
            "Addition train size = 5915\n",
            "Addition test size = 5915\n",
            "Multiplication train size = 10985\n",
            "Multiplication test size = 5915\n",
            "Addition: 35.0% Multiplication: 65.0%\n",
            "Combined dataset = 28730\n",
            "Train size = 16900\n",
            "Test size = 11830\n",
            "train_data.shape = torch.Size([16900, 3])\n",
            "train_labels.shape = torch.Size([16900])\n",
            "test_data.shape = torch.Size([11830, 3])\n",
            "test_labels.shape = torch.Size([11830])\n",
            "train_data[:10] = tensor([[102,   0,   1],\n",
            "        [117,   0, 125],\n",
            "        [121,   1,  24],\n",
            "        [ 88,   0, 100],\n",
            "        [ 42,   1,  35],\n",
            "        [  0,   0, 116],\n",
            "        [ 85,   1,   5],\n",
            "        [  6,   0,  79],\n",
            "        [120,   0,   0],\n",
            "        [ 14,   1,  32]])\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from helpers import get_dataloader\n",
        "\n",
        "max_nums = 130\n",
        "mod_value = 113\n",
        "\n",
        "# The fraction of the data that should be used for training (not all of the rest will be used for testing)\n",
        "train_frac = 0.5\n",
        "\n",
        "# The fraction of the training data that should be addition (the rest will be multiplication)\n",
        "addition_frac = 0.35\n",
        "\n",
        "def create_dataset(max_nums: int, addition: bool, mod_value: int):\n",
        "    data = list()\n",
        "    for a in range(max_nums):\n",
        "        for b in range(max_nums):\n",
        "            if addition:\n",
        "                label = (a + b) % mod_value\n",
        "            else:\n",
        "                label = (a * b) % mod_value\n",
        "            data.append([[a, int(addition), b], label])\n",
        "\n",
        "    return pd.DataFrame(data, columns=[\"input\", \"label\"])\n",
        "\n",
        "addition_df = create_dataset(max_nums, True, mod_value)\n",
        "multiplication_df = create_dataset(max_nums, False, mod_value)\n",
        "\n",
        "print(f\"Addition dataset size = {len(addition_df)}\")\n",
        "print(f\"Multiplication dataset size = {len(multiplication_df)}\")\n",
        "\n",
        "total_train_size = int((len(addition_df) + len(multiplication_df)) * train_frac)\n",
        "\n",
        "print(f\"Total train size = {total_train_size}\")\n",
        "# Calculate the sizes for train datasets based on the desired proportion\n",
        "add_train_size = int(total_train_size * addition_frac)\n",
        "multi_train_size = total_train_size - add_train_size\n",
        "\n",
        "# Determine the size for test datasets (use the remaining data, but ensure equal sizes)\n",
        "test_size = min(len(addition_df) - add_train_size, len(multiplication_df) - multi_train_size)\n",
        "\n",
        "# Create train datasets\n",
        "add_train_df = addition_df.sample(n=add_train_size, random_state=DATA_SEED)\n",
        "multi_train_df = multiplication_df.sample(n=multi_train_size, random_state=DATA_SEED)\n",
        "\n",
        "# Create test datasets with equal size\n",
        "add_test_df = addition_df.drop(add_train_df.index).sample(n=test_size, random_state=DATA_SEED)\n",
        "multi_test_df = multiplication_df.drop(multi_train_df.index).sample(n=test_size, random_state=DATA_SEED)\n",
        "\n",
        "# Print sizes for verification\n",
        "print(f\"Addition train size = {len(add_train_df)}\")\n",
        "print(f\"Addition test size = {len(add_test_df)}\")\n",
        "print(f\"Multiplication train size = {len(multi_train_df)}\")\n",
        "print(f\"Multiplication test size = {len(multi_test_df)}\")\n",
        "\n",
        "print(f\"Addition: {len(add_train_df)/(len(add_train_df) + len(multi_train_df))*100:0.1f}% \\\n",
        "Multiplication: {len(multi_train_df)/(len(add_train_df) + len(multi_train_df))*100:0.1f}%\")\n",
        "\n",
        "# Combine and shuffle the datasets\n",
        "train_df = pd.concat([add_train_df, multi_train_df], ignore_index=True).sample(frac=1, random_state=DATA_SEED).reset_index(drop=True)\n",
        "test_df = pd.concat([add_test_df, multi_test_df], ignore_index=True).sample(frac=1, random_state=DATA_SEED).reset_index(drop=True)\n",
        "\n",
        "print(f\"Combined dataset = {len(train_df) + len(test_df)}\")\n",
        "\n",
        "print(f\"Train size = {len(train_df)}\")\n",
        "print(f\"Test size = {len(test_df)}\")\n",
        "\n",
        "# Create the dataloaders\n",
        "train_loader = get_dataloader(train_df, 1024, shuffle=True)\n",
        "\n",
        "train_data = torch.tensor(train_df['input'].tolist())\n",
        "train_labels = torch.tensor(train_df['label'].tolist())\n",
        "\n",
        "test_data = torch.tensor(test_df['input'].tolist())\n",
        "test_labels = torch.tensor(test_df['label'].tolist())\n",
        "\n",
        "add_test_data = torch.tensor(add_test_df['input'].tolist())\n",
        "add_test_labels = torch.tensor(add_test_df['label'].tolist())\n",
        "\n",
        "multi_test_data = torch.tensor(multi_test_df['input'].tolist())\n",
        "multi_test_labels = torch.tensor(multi_test_df['label'].tolist())\n",
        "\n",
        "print(f\"train_data.shape = {train_data.shape}\")\n",
        "print(f\"train_labels.shape = {train_labels.shape}\")\n",
        "print(f\"test_data.shape = {test_data.shape}\")\n",
        "print(f\"test_labels.shape = {test_labels.shape}\")\n",
        "print(f\"train_data[:10] = {train_data[:10]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  0,   0, 113],\n",
            "        [  0,   1, 113],\n",
            "        [  0,   2, 113],\n",
            "        [  0,   3, 113],\n",
            "        [  0,   4, 113]])\n",
            "torch.Size([12769, 3])\n",
            "torch.Size([12769])\n",
            "tensor([0, 1, 2, 3, 4])\n",
            "tensor([[ 21,  31, 113],\n",
            "        [ 30,  98, 113],\n",
            "        [ 47,  10, 113],\n",
            "        [ 86,  21, 113],\n",
            "        [ 99,  83, 113]])\n",
            "tensor([ 52,  15,  57, 107,  69])\n",
            "torch.Size([5107, 3])\n",
            "tensor([[ 11,  96, 113],\n",
            "        [ 30,  95, 113],\n",
            "        [ 71,  24, 113],\n",
            "        [ 52, 111, 113],\n",
            "        [ 20,  56, 113]])\n",
            "tensor([107,  12,  95,  50,  76])\n",
            "torch.Size([7662, 3])\n"
          ]
        }
      ],
      "source": [
        "ONLY_ADDITION = True\n",
        "\n",
        "if ONLY_ADDITION:\n",
        "    train_frac = 0.4\n",
        "    max_nums = 113\n",
        "    mod_value = 113\n",
        "    a_vector = einops.repeat(torch.arange(max_nums), \"i -> (i j)\", j=max_nums)\n",
        "    b_vector = einops.repeat(torch.arange(max_nums), \"j -> (i j)\", i=max_nums)\n",
        "    equals_vector = einops.repeat(torch.tensor(113), \" -> (i j)\", i=max_nums, j=max_nums)\n",
        "\n",
        "    dataset = torch.stack([a_vector, b_vector, equals_vector], dim=1).to(device)\n",
        "    print(dataset[:5])\n",
        "    print(dataset.shape)\n",
        "\n",
        "    labels = (dataset[:, 0] + dataset[:, 1]) % mod_value\n",
        "    print(labels.shape)\n",
        "    print(labels[:5])\n",
        "\n",
        "    torch.manual_seed(DATA_SEED)\n",
        "    indices = torch.randperm(max_nums*max_nums)\n",
        "    cutoff = int(max_nums*max_nums*train_frac)\n",
        "    train_indices = indices[:cutoff]\n",
        "    test_indices = indices[cutoff:]\n",
        "\n",
        "    train_data = dataset[train_indices]\n",
        "    train_labels = labels[train_indices]\n",
        "    test_data = dataset[test_indices]\n",
        "    test_labels = labels[test_indices]\n",
        "    print(train_data[:5])\n",
        "    print(train_labels[:5])\n",
        "    print(train_data.shape)\n",
        "    print(test_data[:5])\n",
        "    print(test_labels[:5])\n",
        "    print(test_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcd2dy9x2UV6"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "GO7-UYjQ2UV6"
      },
      "outputs": [],
      "source": [
        "\n",
        "cfg = HookedTransformerConfig(\n",
        "    n_layers = 1,\n",
        "    n_heads = 4,\n",
        "    d_model = 128,\n",
        "    d_head = 32,\n",
        "    d_mlp = 512,\n",
        "    act_fn = \"relu\",\n",
        "    normalization_type=\"LN\" if not ONLY_ADDITION else None,\n",
        "    d_vocab=max_nums+1,\n",
        "    d_vocab_out=mod_value,\n",
        "    n_ctx= train_data.shape[1],\n",
        "    init_weights=True,\n",
        "    device=device,\n",
        "    seed = 999,\n",
        ")\n",
        "\n",
        "model = HookedTransformer(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qNAf_pY2UV6"
      },
      "source": [
        "Disable the biases, as we don't need them for this task and it makes things easier to interpret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "9Y6S95XF2UV6"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if \"b_\" in name:\n",
        "        param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQtU2xX02UV6"
      },
      "source": [
        "## Define Optimizer + Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "kt9M_v7a2UV6"
      },
      "outputs": [],
      "source": [
        "# Optimizer config\n",
        "lr = 1e-3\n",
        "wd = 1.\n",
        "betas = (0.90, 0.98)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_loss = 4.734745349574246\n",
            "test_loss = 4.733256843247395\n"
          ]
        }
      ],
      "source": [
        "from helpers import loss_fn\n",
        "\n",
        "train_logits = model(train_data)\n",
        "train_loss = loss_fn(train_logits, train_labels)\n",
        "print(f\"train_loss = {train_loss}\")\n",
        "test_logits = model(test_data)\n",
        "test_loss = loss_fn(test_logits, test_labels)\n",
        "print(f\"test_loss = {test_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOuSwO1U2UV7"
      },
      "source": [
        "## Actually Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c477d3a6463646deb7cf1de99bb0199a"
          ]
        },
        "id": "B78ji8Si2UV7",
        "outputId": "db8171c9-35a3-497a-aa61-a9b8efeee374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train frac = 0.4 Addition frac = 0.35\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65010e60495747d28aa53792cc4a9f3a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/6000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49 Train Loss 4.414182234567524 Test Loss 5.32887969099644\n",
            "Epoch 99 Train Loss 3.171364555716401 Test Loss 6.994213191189663\n",
            "Epoch 149 Train Loss 0.7922345840214302 Test Loss 11.196340732019637\n",
            "Epoch 199 Train Loss 0.04380333607229426 Test Loss 16.94731256866607\n",
            "Epoch 249 Train Loss 0.019859225483423806 Test Loss 17.540105664944658\n",
            "Epoch 299 Train Loss 0.011588217573823258 Test Loss 17.838568775481203\n",
            "Epoch 349 Train Loss 0.006539720886920411 Test Loss 18.2848576868648\n",
            "Epoch 399 Train Loss 0.0037251974747318254 Test Loss 18.752129855354823\n",
            "Epoch 449 Train Loss 0.0021433663225194654 Test Loss 19.23191747614152\n",
            "Epoch 499 Train Loss 0.001238942747030576 Test Loss 19.72905867896849\n",
            "Epoch 549 Train Loss 0.000719551164002961 Test Loss 20.23027658970272\n",
            "Epoch 599 Train Loss 0.0004187748111942845 Test Loss 20.732512613239642\n",
            "Epoch 649 Train Loss 0.0002447288955661891 Test Loss 21.226292675744673\n",
            "Epoch 699 Train Loss 0.00014348120948140758 Test Loss 21.714386015433544\n",
            "Epoch 749 Train Loss 8.443211643062937e-05 Test Loss 22.200646157673425\n",
            "Epoch 799 Train Loss 4.991250067012013e-05 Test Loss 22.689388686946096\n",
            "Epoch 849 Train Loss 2.9653869078612233e-05 Test Loss 23.175706190690487\n",
            "Epoch 899 Train Loss 1.7715950624434604e-05 Test Loss 23.653880421533568\n",
            "Epoch 949 Train Loss 1.0686281861500229e-05 Test Loss 24.116051211092024\n",
            "Epoch 999 Train Loss 6.510072194085068e-06 Test Loss 24.54957862758564\n",
            "Epoch 1049 Train Loss 4.042005797566087e-06 Test Loss 24.94899600004038\n",
            "Epoch 1099 Train Loss 2.573737724919817e-06 Test Loss 25.302079419543226\n",
            "Epoch 1149 Train Loss 1.6960843943239582e-06 Test Loss 25.59968606963701\n",
            "Epoch 1199 Train Loss 1.1671400607943332e-06 Test Loss 25.830835828110438\n",
            "Epoch 1249 Train Loss 8.476343079812179e-07 Test Loss 25.983678513960392\n",
            "Epoch 1299 Train Loss 6.535314808827899e-07 Test Loss 26.063275001079802\n",
            "Epoch 1349 Train Loss 5.348702253021171e-07 Test Loss 26.06388803623658\n",
            "Epoch 1399 Train Loss 4.641036114529389e-07 Test Loss 25.994140472214568\n",
            "Epoch 1449 Train Loss 4.2341510224751175e-07 Test Loss 25.86324497580696\n",
            "Epoch 1499 Train Loss 4.0086117373717846e-07 Test Loss 25.68549486061588\n",
            "Epoch 1549 Train Loss 3.8983004027619625e-07 Test Loss 25.477065912510703\n",
            "Epoch 1599 Train Loss 3.8436844177224815e-07 Test Loss 25.256939641897656\n",
            "Epoch 1649 Train Loss 3.811721752707673e-07 Test Loss 25.032762077340774\n",
            "Epoch 1699 Train Loss 3.795148982783438e-07 Test Loss 24.81106266704082\n",
            "Epoch 1749 Train Loss 3.785956793944861e-07 Test Loss 24.59267449333154\n",
            "Epoch 1799 Train Loss 3.776989370404947e-07 Test Loss 24.37911208875448\n",
            "Epoch 1849 Train Loss 3.769095341602737e-07 Test Loss 24.163176887261308\n",
            "Epoch 1899 Train Loss 3.760650458411349e-07 Test Loss 23.953154290360523\n",
            "Epoch 1949 Train Loss 3.7520711351176465e-07 Test Loss 23.74078506342754\n",
            "Epoch 1999 Train Loss 3.7427549688205295e-07 Test Loss 23.528485369432946\n",
            "Epoch 2049 Train Loss 3.7364816589811605e-07 Test Loss 23.31607105317256\n",
            "Epoch 2099 Train Loss 3.7254656725453274e-07 Test Loss 23.106795528654303\n",
            "Epoch 2149 Train Loss 3.718906556888428e-07 Test Loss 22.89473298234544\n",
            "Epoch 2199 Train Loss 3.7064032645123426e-07 Test Loss 22.681326411658567\n",
            "Epoch 2249 Train Loss 3.6990757690751814e-07 Test Loss 22.46754791732729\n",
            "Epoch 2299 Train Loss 3.687657696782773e-07 Test Loss 22.254436526373098\n",
            "Epoch 2349 Train Loss 3.673904550771633e-07 Test Loss 22.041023154324915\n",
            "Epoch 2399 Train Loss 3.666085388909355e-07 Test Loss 21.82725533639018\n",
            "Epoch 2449 Train Loss 3.653748341207209e-07 Test Loss 21.61193333101496\n",
            "Epoch 2499 Train Loss 3.6454183253236014e-07 Test Loss 21.396799345292536\n",
            "Epoch 2549 Train Loss 3.633865053672169e-07 Test Loss 21.178390525070498\n",
            "Epoch 2599 Train Loss 3.6253984470253355e-07 Test Loss 20.95905651935922\n",
            "Epoch 2649 Train Loss 3.613343906843233e-07 Test Loss 20.73978612555432\n",
            "Epoch 2699 Train Loss 3.603018858179781e-07 Test Loss 20.516358306146525\n",
            "Epoch 2749 Train Loss 3.590668815682091e-07 Test Loss 20.28674698456422\n",
            "Epoch 2799 Train Loss 3.5791168277893125e-07 Test Loss 20.0548642536226\n",
            "Epoch 2849 Train Loss 3.567471904521452e-07 Test Loss 19.81960089010915\n",
            "Epoch 2899 Train Loss 3.55539942638424e-07 Test Loss 19.58144564764666\n",
            "Epoch 2949 Train Loss 3.5445812119167723e-07 Test Loss 19.33995163866212\n",
            "Epoch 2999 Train Loss 3.530891539190354e-07 Test Loss 19.093739966238235\n",
            "Epoch 3049 Train Loss 3.518880445630535e-07 Test Loss 18.841326098877904\n",
            "Epoch 3099 Train Loss 3.505857582101522e-07 Test Loss 18.58044293074638\n",
            "Epoch 3149 Train Loss 3.49411245353682e-07 Test Loss 18.316485329639047\n",
            "Epoch 3199 Train Loss 3.4807797877372865e-07 Test Loss 18.04427762122477\n",
            "Epoch 3249 Train Loss 3.467043088058395e-07 Test Loss 17.77125850314474\n",
            "Epoch 3299 Train Loss 3.452781810274834e-07 Test Loss 17.49487237746242\n",
            "Epoch 3349 Train Loss 3.4388502207828976e-07 Test Loss 17.207500610747097\n",
            "Epoch 3399 Train Loss 3.4251565996311564e-07 Test Loss 16.91171611350963\n",
            "Epoch 3449 Train Loss 3.4107957734572013e-07 Test Loss 16.612509428455255\n",
            "Epoch 3499 Train Loss 3.3994884888666353e-07 Test Loss 16.312107141279643\n",
            "Epoch 3549 Train Loss 3.384888022401068e-07 Test Loss 16.012014052280364\n",
            "Epoch 3599 Train Loss 3.3668720623304694e-07 Test Loss 15.70344671124485\n",
            "Epoch 3649 Train Loss 3.3544271915460957e-07 Test Loss 15.388986209539963\n",
            "Epoch 3699 Train Loss 3.3408051684495195e-07 Test Loss 15.072157722598247\n",
            "Epoch 3749 Train Loss 3.3244151308852263e-07 Test Loss 14.751195103983179\n",
            "Epoch 3799 Train Loss 3.306543953848354e-07 Test Loss 14.418027480643536\n",
            "Epoch 3849 Train Loss 3.2891850753154774e-07 Test Loss 14.079965632953254\n",
            "Epoch 3899 Train Loss 3.2737873920333014e-07 Test Loss 13.741748900137946\n",
            "Epoch 3949 Train Loss 3.2542164958923456e-07 Test Loss 13.397731295304563\n",
            "Epoch 3999 Train Loss 3.239833769645326e-07 Test Loss 13.045122834753787\n",
            "Epoch 4049 Train Loss 3.221407157404082e-07 Test Loss 12.685689788315209\n",
            "Epoch 4099 Train Loss 3.202896863291798e-07 Test Loss 12.321110322901362\n",
            "Epoch 4149 Train Loss 3.1829617667145354e-07 Test Loss 11.948302685617188\n",
            "Epoch 4199 Train Loss 3.1661186190001857e-07 Test Loss 11.568662040711736\n",
            "Epoch 4249 Train Loss 3.1431458533095575e-07 Test Loss 11.1854456579006\n",
            "Epoch 4299 Train Loss 3.1239303008829e-07 Test Loss 10.797156087036132\n",
            "Epoch 4349 Train Loss 3.0992985244008197e-07 Test Loss 10.40247760802063\n",
            "Epoch 4399 Train Loss 3.078254367573664e-07 Test Loss 10.002973979197618\n",
            "Epoch 4449 Train Loss 3.0545668657888866e-07 Test Loss 9.599218660101606\n",
            "Epoch 4499 Train Loss 3.032859756138945e-07 Test Loss 9.187718690786644\n",
            "Epoch 4549 Train Loss 3.0044406138480566e-07 Test Loss 8.765924792340396\n",
            "Epoch 4599 Train Loss 2.9777600639062266e-07 Test Loss 8.337076782342319\n",
            "Epoch 4649 Train Loss 2.948273603362456e-07 Test Loss 7.895589205620646\n",
            "Epoch 4699 Train Loss 2.9162345250276285e-07 Test Loss 7.448023344386483\n",
            "Epoch 4749 Train Loss 2.887266520064851e-07 Test Loss 6.994929360745461\n",
            "Epoch 4799 Train Loss 2.85302080916031e-07 Test Loss 6.541331286300855\n",
            "Epoch 4849 Train Loss 2.818096584980717e-07 Test Loss 6.077881395321187\n",
            "Epoch 4899 Train Loss 2.776546149610871e-07 Test Loss 5.604192775446641\n",
            "Epoch 4949 Train Loss 2.7306578770892847e-07 Test Loss 5.121794467271446\n",
            "Epoch 4999 Train Loss 2.6841683816221267e-07 Test Loss 4.6292762232226785\n",
            "Epoch 5049 Train Loss 2.6395064646768574e-07 Test Loss 4.14170247630704\n",
            "Epoch 5099 Train Loss 2.584273624636304e-07 Test Loss 3.6583033717368463\n",
            "Epoch 5149 Train Loss 2.5268359320977387e-07 Test Loss 3.1789998535777273\n",
            "Epoch 5199 Train Loss 2.4713520032619284e-07 Test Loss 2.7145099929861347\n",
            "Epoch 5249 Train Loss 2.4138707390881727e-07 Test Loss 2.276701044018717\n",
            "Epoch 5299 Train Loss 2.359655966658692e-07 Test Loss 1.8731040201078077\n",
            "Epoch 5349 Train Loss 2.3067075019792387e-07 Test Loss 1.5129448973687474\n",
            "Epoch 5399 Train Loss 2.2541573609025963e-07 Test Loss 1.198943415439549\n",
            "Epoch 5449 Train Loss 2.2018471034843454e-07 Test Loss 0.933033034870082\n",
            "Epoch 5499 Train Loss 2.1507518020098872e-07 Test Loss 0.7115344180666875\n",
            "Epoch 5549 Train Loss 2.099098131759174e-07 Test Loss 0.5327024650588574\n",
            "Epoch 5599 Train Loss 2.047607944200785e-07 Test Loss 0.392932231499593\n",
            "Epoch 5649 Train Loss 1.9968120039808607e-07 Test Loss 0.2855470204772154\n",
            "Epoch 5699 Train Loss 1.9489361416846427e-07 Test Loss 0.20636682187089875\n",
            "Epoch 5749 Train Loss 1.8986520610132742e-07 Test Loss 0.14774660742931092\n",
            "Epoch 5799 Train Loss 1.8550581511780061e-07 Test Loss 0.10515074428048665\n",
            "Epoch 5849 Train Loss 1.8112222277023825e-07 Test Loss 0.07434343045564017\n",
            "Epoch 5899 Train Loss 1.769832641173924e-07 Test Loss 0.052632856307211644\n",
            "Epoch 5949 Train Loss 1.736091968269543e-07 Test Loss 0.03707217421074407\n",
            "Epoch 5999 Train Loss 1.7034230184098093e-07 Test Loss 0.026338607933962175\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 6000\n",
        "checkpoint_every = 50\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "add_test_losses = []\n",
        "multi_test_losses = []\n",
        "model_checkpoints = []\n",
        "checkpoint_epochs = []\n",
        "if TRAIN_MODEL:\n",
        "    print(f\"Train frac = {train_frac} Addition frac = {addition_frac}\")\n",
        "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
        "        if not ONLY_ADDITION:\n",
        "            model.train()\n",
        "        train_logits = model(train_data)\n",
        "        train_loss = loss_fn(train_logits, train_labels)\n",
        "        train_loss.backward()\n",
        "        train_losses.append(train_loss.item())\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if not ONLY_ADDITION:\n",
        "            model.eval()\n",
        "        with torch.inference_mode():\n",
        "            # General test loss\n",
        "            test_logits = model(test_data)\n",
        "            test_loss = loss_fn(test_logits, test_labels)\n",
        "            test_losses.append(test_loss.item())\n",
        "\n",
        "            if (epoch+1) % 10 == 0 and not ONLY_ADDITION:\n",
        "                # Addition test loss\n",
        "                add_test_logits = model(add_test_data)\n",
        "                add_test_loss = loss_fn(add_test_logits, add_test_labels)\n",
        "                add_test_losses.append(add_test_loss.item())\n",
        "\n",
        "                # Multiplication test loss\n",
        "                multi_test_logits = model(multi_test_data)\n",
        "                multi_test_loss = loss_fn(multi_test_logits, multi_test_labels)\n",
        "                multi_test_losses.append(multi_test_loss.item())\n",
        "\n",
        "        if ((epoch+1)%checkpoint_every)==0:\n",
        "            checkpoint_epochs.append(epoch)\n",
        "            model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
        "            \n",
        "            if ONLY_ADDITION:\n",
        "                print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}\")\n",
        "            else:\n",
        "                print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()} Add Test Loss {add_test_loss.item()} Multi Test Loss {multi_test_loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "UrvOo2Qs2UV7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(train_losses) = 6000 len(test_losses) = 6000 len(model_checkpoints) = 120\n"
          ]
        }
      ],
      "source": [
        "PTH_LOCATION = \"../saves/check_point_50/grokking_addition.pth\"\n",
        "if TRAIN_MODEL:\n",
        "    # Create the directory if it does not exist\n",
        "    os.makedirs(Path(PTH_LOCATION).parent, exist_ok=True)\n",
        "    \n",
        "    print(f\"len(train_losses) = {len(train_losses)} len(test_losses) = {len(test_losses)} len(model_checkpoints) = {len(model_checkpoints)}\")\n",
        "\n",
        "    if ONLY_ADDITION:\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\":model.state_dict(),\n",
        "                \"config\": model.cfg,\n",
        "                \"checkpoints\": model_checkpoints,\n",
        "                \"checkpoint_epochs\": checkpoint_epochs,\n",
        "                \"test_losses\": test_losses,\n",
        "                \"train_losses\": train_losses,\n",
        "                \"max_nums\": max_nums,\n",
        "                \"mod_value\": mod_value,\n",
        "                \"train_frac\": train_frac,\n",
        "                \"learning_rate\": lr,\n",
        "                \"weight_decay\": wd,\n",
        "                \"betas\": betas,\n",
        "                \"train_data\": train_data,\n",
        "                \"train_labels\": train_labels,\n",
        "                \"test_data\": test_data,\n",
        "                \"test_labels\": test_labels,\n",
        "            },\n",
        "            PTH_LOCATION)\n",
        "    else:\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\":model.state_dict(),\n",
        "                \"config\": model.cfg,\n",
        "                \"checkpoints\": model_checkpoints,\n",
        "                \"checkpoint_epochs\": checkpoint_epochs,\n",
        "                \"test_losses\": test_losses,\n",
        "                \"train_losses\": train_losses,\n",
        "                \"add_test_losses\": add_test_losses,\n",
        "                \"multi_test_losses\": multi_test_losses,\n",
        "                \"max_nums\": max_nums,\n",
        "                \"mod_value\": mod_value,\n",
        "                \"train_frac\": train_frac,\n",
        "                \"addition_frac\": addition_frac,\n",
        "                \"learning_rate\": lr,\n",
        "                \"weight_decay\": wd,\n",
        "                \"betas\": betas,\n",
        "                \"train_data\": train_data,\n",
        "                \"train_labels\": train_labels,\n",
        "                \"test_data\": test_data,\n",
        "                \"test_labels\": test_labels,\n",
        "            },\n",
        "            PTH_LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "id": "tx5DVueg2UV7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_frac = 0.5 addition_frac = 0.5\n",
            "len(train_losses) = 4000 len(test_losses) = 4000 len(model_checkpoints) = 40\n"
          ]
        }
      ],
      "source": [
        "LOAD_LOCATION = \"../saves/grokking_add_multi_0.5.pth\"\n",
        "if not TRAIN_MODEL:\n",
        "    cached_data = torch.load(LOAD_LOCATION, weights_only=False)\n",
        "    model.load_state_dict(cached_data['model'])\n",
        "    model_checkpoints = cached_data[\"checkpoints\"]\n",
        "    checkpoint_epochs = cached_data[\"checkpoint_epochs\"]\n",
        "    test_losses = cached_data['test_losses']\n",
        "    train_losses = cached_data['train_losses']\n",
        "    add_test_losses = cached_data['add_test_losses']\n",
        "    multi_test_losses = cached_data['multi_test_losses']\n",
        "    max_nums = cached_data['max_nums']\n",
        "    mod_value = cached_data['mod_value']\n",
        "    train_frac = cached_data['train_frac']\n",
        "    addition_frac = cached_data['addition_frac']\n",
        "    print(f\"train_frac = {train_frac} addition_frac = {addition_frac}\")\n",
        "    print(f\"len(train_losses) = {len(train_losses)} len(test_losses) = {len(test_losses)} len(model_checkpoints) = {len(model_checkpoints)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_loss = 0.10750396571662794\n"
          ]
        }
      ],
      "source": [
        "test_logits = model(test_data)\n",
        "test_loss = loss_fn(test_logits, test_labels)\n",
        "print(f\"test_loss = {test_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHA9-QD52UV7"
      },
      "source": [
        "## Show Model Training Statistics, Check that it groks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "mHu_Yz8i2UV7",
        "outputId": "ff53e647-20f1-4a7d-8dae-f680164305f0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.35.0.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>                            <div id=\"dc31b3f5-4823-4963-84ba-94c0a3bb6255\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"dc31b3f5-4823-4963-84ba-94c0a3bb6255\")) {                    Plotly.newPlot(                        \"dc31b3f5-4823-4963-84ba-94c0a3bb6255\",                        [{\"hovertemplate\":\"Color=train loss\\u003cbr\\u003eEpoch=%{x}\\u003cbr\\u003eLoss=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"train loss\",\"showlegend\":true,\"x\":[0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,750,760,770,780,790,800,810,820,830,840,850,860,870,880,890,900,910,920,930,940,950,960,970,980,990,1000,1010,1020,1030,1040,1050,1060,1070,1080,1090,1100,1110,1120,1130,1140,1150,1160,1170,1180,1190,1200,1210,1220,1230,1240,1250,1260,1270,1280,1290,1300,1310,1320,1330,1340,1350,1360,1370,1380,1390,1400,1410,1420,1430,1440,1450,1460,1470,1480,1490,1500,1510,1520,1530,1540,1550,1560,1570,1580,1590,1600,1610,1620,1630,1640,1650,1660,1670,1680,1690,1700,1710,1720,1730,1740,1750,1760,1770,1780,1790,1800,1810,1820,1830,1840,1850,1860,1870,1880,1890,1900,1910,1920,1930,1940,1950,1960,1970,1980,1990,2000,2010,2020,2030,2040,2050,2060,2070,2080,2090,2100,2110,2120,2130,2140,2150,2160,2170,2180,2190,2200,2210,2220,2230,2240,2250,2260,2270,2280,2290,2300,2310,2320,2330,2340,2350,2360,2370,2380,2390,2400,2410,2420,2430,2440,2450,2460,2470,2480,2490,2500,2510,2520,2530,2540,2550,2560,2570,2580,2590,2600,2610,2620,2630,2640,2650,2660,2670,2680,2690,2700,2710,2720,2730,2740,2750,2760,2770,2780,2790,2800,2810,2820,2830,2840,2850,2860,2870,2880,2890,2900,2910,2920,2930,2940,2950,2960,2970,2980,2990,3000,3010,3020,3030,3040,3050,3060,3070,3080,3090,3100,3110,3120,3130,3140,3150,3160,3170,3180,3190,3200,3210,3220,3230,3240,3250,3260,3270,3280,3290,3300,3310,3320,3330,3340,3350,3360,3370,3380,3390,3400,3410,3420,3430,3440,3450,3460,3470,3480,3490,3500,3510,3520,3530,3540,3550,3560,3570,3580,3590,3600,3610,3620,3630,3640,3650,3660,3670,3680,3690,3700,3710,3720,3730,3740,3750,3760,3770,3780,3790,3800,3810,3820,3830,3840,3850,3860,3870,3880,3890,3900,3910,3920,3930,3940,3950,3960,3970,3980,3990,4000,4010,4020,4030,4040,4050,4060,4070,4080,4090,4100,4110,4120,4130,4140,4150,4160,4170,4180,4190,4200,4210,4220,4230,4240,4250,4260,4270,4280,4290,4300,4310,4320,4330,4340,4350,4360,4370,4380,4390,4400,4410,4420,4430,4440,4450,4460,4470,4480,4490,4500,4510,4520,4530,4540,4550,4560,4570,4580,4590,4600,4610,4620,4630,4640,4650,4660,4670,4680,4690,4700,4710,4720,4730,4740,4750,4760,4770,4780,4790,4800,4810,4820,4830,4840,4850,4860,4870,4880,4890,4900,4910,4920,4930,4940,4950,4960,4970,4980,4990,5000,5010,5020,5030,5040,5050,5060,5070,5080,5090,5100,5110,5120,5130,5140,5150,5160,5170,5180,5190,5200,5210,5220,5230,5240,5250,5260,5270,5280,5290,5300,5310,5320,5330,5340,5350,5360,5370,5380,5390,5400,5410,5420,5430,5440,5450,5460,5470,5480,5490,5500,5510,5520,5530,5540,5550,5560,5570,5580,5590,5600,5610,5620,5630,5640,5650,5660,5670,5680,5690,5700,5710,5720,5730,5740,5750,5760,5770,5780,5790,5800,5810,5820,5830,5840,5850,5860,5870,5880,5890,5900,5910,5920,5930,5940,5950,5960,5970,5980,5990],\"xaxis\":\"x\",\"y\":[4.734745349574246,4.7082530758923085,4.678782820938124,4.606474950881239,4.509304155438879,4.40279161936525,4.274954153757029,4.106140806382331,3.8714474455882173,3.5462549747306547,3.125587433217699,2.64056467764763,2.1231449438937515,1.6215349252352025,1.1603527652139645,0.7555557771587385,0.4350788805652517,0.2211846745879831,0.11084694096976042,0.0630028525228637,0.04239246819644255,0.0326708995027785,0.027458768577380676,0.024180456039479315,0.021738580208874612,0.019661407941876402,0.017766126892048987,0.0159950322840206,0.014349243726381425,0.012835873954298783,0.011457822050052928,0.010216272609975327,0.009108372480308134,0.00812350738881178,0.007247911928829714,0.006465728063833679,0.005774373974880798,0.005156792287629961,0.004606693995801484,0.004118728877749859,0.003684512247872794,0.0032966279053501606,0.002950089379178534,0.0026418187496227547,0.0023665229416744656,0.002120794224144273,0.0018997251924244723,0.0017018231275127032,0.0015250742817655809,0.0013670042393473885,0.0012255649601759623,0.0010992524886730018,0.0009862098792904957,0.0008844335475328423,0.0007934040842115402,0.0007117666518761778,0.0006387116451952306,0.000573094400395058,0.0005143197408943643,0.00046148620023364086,0.0004143585257340941,0.00037196157477673025,0.00033387584548250207,0.00029989961240918105,0.00026943974025411404,0.00024208609436934104,0.000217549160572399,0.0001954355435329309,0.00017565292353613653,0.00015791839418110006,0.00014193639612081205,0.00012764316752974673,0.0001147679982403214,0.00010322213325306354,9.288261327866541e-05,8.355097361835697e-05,7.522514221054098e-05,6.7669797466036e-05,6.092256627882998e-05,5.485012004877053e-05,4.938994961639014e-05,4.448597746126097e-05,4.007061937612603e-05,3.611609723717822e-05,3.254618926993733e-05,2.934656790822979e-05,2.6457656711188658e-05,2.385920369668081e-05,2.1528007821353873e-05,1.9417755689441457e-05,1.753716600671206e-05,1.583425157520526e-05,1.4304933908843983e-05,1.2932967506131567e-05,1.1699892614892282e-05,1.0576399896658326e-05,9.570845192772411e-06,8.664188233177657e-06,7.846084708537963e-06,7.112434789960608e-06,6.448112748332638e-06,5.850878507797598e-06,5.3143081916345715e-06,4.831546028221916e-06,4.3972833330782055e-06,4.004964143051272e-06,3.6515180315578283e-06,3.3324694603823253e-06,3.0447666418385364e-06,2.785529331271646e-06,2.5520895447998107e-06,2.3405660330875993e-06,2.150121912636261e-06,1.97804235501689e-06,1.8227978245769566e-06,1.6825537630223147e-06,1.555763775174976e-06,1.4415490196137174e-06,1.338051996928757e-06,1.2442642760893792e-06,1.1590523477118154e-06,1.0827021730890276e-06,1.013286163471589e-06,9.506963638833227e-07,8.940810994867401e-07,8.426567280107765e-07,7.965026974975994e-07,7.541240991995729e-07,7.159369952938827e-07,6.813847438316478e-07,6.503942502239657e-07,6.220980795571339e-07,5.964467348742317e-07,5.731511226584129e-07,5.52171680871146e-07,5.331411095159473e-07,5.159598424148748e-07,5.006981891074115e-07,4.869504062472519e-07,4.7452422658923256e-07,4.631490636212007e-07,4.530737653869395e-07,4.4409627190180934e-07,4.3603868843803127e-07,4.288702532938377e-07,4.2290738200525706e-07,4.169184890974033e-07,4.1236112415108037e-07,4.077000091256784e-07,4.0435459267322075e-07,4.005792446175931e-07,3.981137337798055e-07,3.9521599014609087e-07,3.9324759947216367e-07,3.910164200632835e-07,3.8944559347777114e-07,3.8804568591740196e-07,3.868720604013869e-07,3.854819891066515e-07,3.8457039726239383e-07,3.841803997642283e-07,3.8311944792471615e-07,3.8290215982611046e-07,3.8198566050970634e-07,3.818523855594182e-07,3.8102997732434023e-07,3.810328037769207e-07,3.8032333878757033e-07,3.803513883812362e-07,3.800394968211858e-07,3.794574804230479e-07,3.795146224880539e-07,3.7906730353237787e-07,3.788800563036326e-07,3.7885793484999456e-07,3.787398403005509e-07,3.7850162109020574e-07,3.7843069964466193e-07,3.783060308465852e-07,3.7784102540229153e-07,3.778181507598868e-07,3.776337247310304e-07,3.7733572727383773e-07,3.7750814026815356e-07,3.7700094018938833e-07,3.769845751560656e-07,3.768995606737365e-07,3.765232771968047e-07,3.765058937312036e-07,3.7658763808423146e-07,3.760160604899709e-07,3.7602740666888e-07,3.7592486170050927e-07,3.757277549926855e-07,3.755906861043017e-07,3.751561902568104e-07,3.753366392660619e-07,3.7504953576418145e-07,3.746599486349221e-07,3.7495226769056126e-07,3.742962521231161e-07,3.7408962940277165e-07,3.7410893293425923e-07,3.7368846031667574e-07,3.7373275194527016e-07,3.7350971676148954e-07,3.7316948248167717e-07,3.7299222550258735e-07,3.7311140687645575e-07,3.7262760834485107e-07,3.727699243967187e-07,3.7224652517627604e-07,3.7242945779504667e-07,3.718021899609383e-07,3.7183414492107973e-07,3.71747044791501e-07,3.7123399328672447e-07,3.712607888079551e-07,3.7117533595755824e-07,3.707012871538104e-07,3.705082485778004e-07,3.706029251397883e-07,3.7016801763427817e-07,3.6988133016926247e-07,3.6994084110857583e-07,3.6970590056350486e-07,3.69273254316808e-07,3.6923768214517123e-07,3.688552153127079e-07,3.6868113307561524e-07,3.6872051149953586e-07,3.68261354903009e-07,3.682589573280508e-07,3.6777481593535796e-07,3.6788704031204826e-07,3.6735230903106533e-07,3.6733463859008406e-07,3.6728740290374345e-07,3.667241379745359e-07,3.667970888242598e-07,3.6636394479027784e-07,3.663022221641449e-07,3.664612813842045e-07,3.657587370181768e-07,3.6584692684069526e-07,3.655298273936731e-07,3.652586651336142e-07,3.65072537021324e-07,3.649771443121081e-07,3.646300865401049e-07,3.6462950577578356e-07,3.6454098473795573e-07,3.6400401289306037e-07,3.6391693425620257e-07,3.63746226151638e-07,3.633164942228191e-07,3.6325566317589367e-07,3.631235692227556e-07,3.628906035090573e-07,3.6253419718417597e-07,3.626505348439331e-07,3.6223152311759634e-07,3.6197994843979e-07,3.619131769443312e-07,3.6159181315555915e-07,3.6124748240382154e-07,3.6110544971230314e-07,3.610293072753813e-07,3.60593407886245e-07,3.6064720201372713e-07,3.6019041005804897e-07,3.5993556818104376e-07,3.5975631946087293e-07,3.596649319903997e-07,3.592898221037132e-07,3.5912044028586336e-07,3.59080133800458e-07,3.584901673006544e-07,3.58478201635906e-07,3.5805771482841854e-07,3.5802762822190004e-07,3.5758337044267736e-07,3.57568625162766e-07,3.5728226562996725e-07,3.568205836080742e-07,3.568418497811226e-07,3.5649760720164636e-07,3.561648624556797e-07,3.5591460014614534e-07,3.5598863627779454e-07,3.55447299319973e-07,3.554964885361308e-07,3.550103159004394e-07,3.548000203247694e-07,3.5498780866373287e-07,3.5432020602076826e-07,3.540628521033987e-07,3.539690686936835e-07,3.539711175598873e-07,3.532355028633352e-07,3.532115142688687e-07,3.5277840989910944e-07,3.5257000830000124e-07,3.5247189504642256e-07,3.522030423350415e-07,3.5195227147008206e-07,3.517389568126009e-07,3.515969730364854e-07,3.510248098679685e-07,3.5081826343306064e-07,3.5058575683955854e-07,3.505911097931726e-07,3.502231038620848e-07,3.4991340918762645e-07,3.4963006859242524e-07,3.4933483030565916e-07,3.4889246376117307e-07,3.486905257420565e-07,3.485723155604397e-07,3.483474885653506e-07,3.4805274131852403e-07,3.480856204675241e-07,3.475205875279089e-07,3.474116459745827e-07,3.4696700089718816e-07,3.467310496401349e-07,3.4666708240507437e-07,3.4616723399640446e-07,3.460714182637453e-07,3.4565161748916165e-07,3.453060171965747e-07,3.451977501159485e-07,3.448362345140858e-07,3.445734019844322e-07,3.4413817616626347e-07,3.4395260449215227e-07,3.4368390910660964e-07,3.433050443278921e-07,3.432049279885707e-07,3.427493842759639e-07,3.4254193184360126e-07,3.4231303407722145e-07,3.4193371929384567e-07,3.418348463114338e-07,3.413815202513565e-07,3.410998285983507e-07,3.409545435020748e-07,3.407307701709963e-07,3.404022433832057e-07,3.402784659169297e-07,3.398060667674419e-07,3.3948674733774654e-07,3.3930746368653285e-07,3.3886311745033437e-07,3.38757887546157e-07,3.3844028845046736e-07,3.378676589999897e-07,3.378391061894592e-07,3.373102388439666e-07,3.3699846108136724e-07,3.366857461129218e-07,3.3662063656572995e-07,3.361814194547097e-07,3.361333052141133e-07,3.356917604677516e-07,3.3554323956758116e-07,3.350535090031404e-07,3.3482216220202936e-07,3.344488327838626e-07,3.34244547163788e-07,3.33961502857561e-07,3.3364246254667506e-07,3.3342794802475566e-07,3.3305685629349053e-07,3.3268170106634546e-07,3.324599439816275e-07,3.3193641431033806e-07,3.3165083427380905e-07,3.3125934174293933e-07,3.3081654121692905e-07,3.30711570126317e-07,3.30269133849884e-07,3.2992394073398055e-07,3.297467086505911e-07,3.29282742565206e-07,3.288435828320701e-07,3.285100012060226e-07,3.2819797946260425e-07,3.2791056698583144e-07,3.2760487399446665e-07,3.2737608386868956e-07,3.269811658594892e-07,3.2653136827665704e-07,3.262006700822536e-07,3.257938911007904e-07,3.253837308054871e-07,3.2511777537325615e-07,3.2474480094174185e-07,3.2437996941057174e-07,3.242498822124303e-07,3.2390929874327156e-07,3.234426487833435e-07,3.231126110039131e-07,3.2290812701935296e-07,3.225440454042938e-07,3.220736648797793e-07,3.218283556936579e-07,3.213386405737183e-07,3.2107597231606453e-07,3.208023278278995e-07,3.2025393058630894e-07,3.199638161243422e-07,3.1939804713579995e-07,3.191595183962754e-07,3.1872316372386305e-07,3.182830840084073e-07,3.1801106921536325e-07,3.1756141688936685e-07,3.1720881277083206e-07,3.1681446670820927e-07,3.1658232687729125e-07,3.160117976830072e-07,3.155467499653275e-07,3.152080794375741e-07,3.147179415946538e-07,3.142410659656263e-07,3.139399183784688e-07,3.134273860411484e-07,3.130034124094536e-07,3.127327977027368e-07,3.1230281562566375e-07,3.1175641692620994e-07,3.1145487607618093e-07,3.1083103864246374e-07,3.103682039171429e-07,3.09888023034694e-07,3.0953849083175674e-07,3.0905892913359443e-07,3.0882468626110133e-07,3.0830839956670136e-07,3.077256712792742e-07,3.0718166586578565e-07,3.06754936915695e-07,3.0639099883554395e-07,3.058962061947085e-07,3.054156585603593e-07,3.0504724689134295e-07,3.04473760827022e-07,3.0399987577912943e-07,3.0355539838288916e-07,3.0312177615397144e-07,3.0256308147504004e-07,3.019606255763805e-07,3.012734138379674e-07,3.0079192568528044e-07,3.003617293923284e-07,2.99823972955608e-07,2.9945445146728944e-07,2.9892375287578134e-07,2.9828006004624096e-07,2.9775731853210334e-07,2.9711805397477407e-07,2.96458582091439e-07,2.958051665033284e-07,2.952674264520341e-07,2.9476732383956446e-07,2.941317923526307e-07,2.935662978375097e-07,2.928277876054613e-07,2.922251388822274e-07,2.9152288825382565e-07,2.9096972754086795e-07,2.9040681934439245e-07,2.896874616228482e-07,2.8921539885746164e-07,2.885790449503441e-07,2.878983812021319e-07,2.873779583491095e-07,2.8659063113463005e-07,2.860289385632298e-07,2.852866175984418e-07,2.8454112040144504e-07,2.8390530395561343e-07,2.8309926446690284e-07,2.8246235033177693e-07,2.8177045048206763e-07,2.8096988695483855e-07,2.8000655066387865e-07,2.792146643340601e-07,2.7837723118394903e-07,2.775975094503991e-07,2.767751898327161e-07,2.757845598462071e-07,2.748537128857896e-07,2.739996975262087e-07,2.7298021232557053e-07,2.7212439225007597e-07,2.7115406255754637e-07,2.702559523805576e-07,2.692900958378143e-07,2.6829167192955736e-07,2.673891115555697e-07,2.666376164292608e-07,2.656834076790674e-07,2.647366528896978e-07,2.638634924804649e-07,2.629009588741547e-07,2.6166670875293374e-07,2.605661673200799e-07,2.593877909074891e-07,2.583204753485261e-07,2.572283227615631e-07,2.5595481534479806e-07,2.547980466131289e-07,2.536550994009594e-07,2.5252391965429115e-07,2.514790427437228e-07,2.504061264775811e-07,2.4926385626627355e-07,2.481451225014314e-07,2.469987151795339e-07,2.4590870405870745e-07,2.446887617402537e-07,2.4354123935864054e-07,2.423821462912868e-07,2.41253428619638e-07,2.4004985166183705e-07,2.3895180258033036e-07,2.379698494271667e-07,2.368565994514909e-07,2.3588890910816186e-07,2.3494617307253126e-07,2.3385411378688157e-07,2.3268739160834908e-07,2.3164396725622126e-07,2.305037828425907e-07,2.293168775221182e-07,2.282432923018862e-07,2.272882411008224e-07,2.26188270937543e-07,2.252921370802674e-07,2.242669059231689e-07,2.233963944928116e-07,2.2223982845770198e-07,2.2109166967932963e-07,2.200566821472874e-07,2.1893684796801244e-07,2.1805732729459888e-07,2.170726456541351e-07,2.160155522855715e-07,2.1499742890989287e-07,2.140332429262968e-07,2.1279038857642815e-07,2.118601408733965e-07,2.1075265998912666e-07,2.0982489017625306e-07,2.0894316160901303e-07,2.0789347772830442e-07,2.0677525538097202e-07,2.057529507896762e-07,2.0462099918246004e-07,2.0348432876776675e-07,2.02414316529477e-07,2.0147647625128268e-07,2.0053500238245086e-07,1.996113456652249e-07,1.986005257145165e-07,1.9753788906194148e-07,1.9671272815445544e-07,1.9577689017106105e-07,1.9472685164899917e-07,1.9366496683091274e-07,1.9267592016644978e-07,1.9162517005594473e-07,1.9080259872653498e-07,1.8974145482852828e-07,1.88897586356606e-07,1.880602276194034e-07,1.8724731338599843e-07,1.8621543833965183e-07,1.853508400781045e-07,1.8460826898674958e-07,1.836559153080591e-07,1.828319009147499e-07,1.8196229247181665e-07,1.8103582581707517e-07,1.8028909903658388e-07,1.7943810791890704e-07,1.7851275602725616e-07,1.7791784086792032e-07,1.76938704956709e-07,1.76154895170706e-07,1.755676566850449e-07,1.7491118144238476e-07,1.742668042112451e-07,1.7355882446591052e-07,1.7290320788706513e-07,1.722330093883401e-07,1.716495002370921e-07,1.7089249219742748e-07],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"Color=test loss\\u003cbr\\u003eEpoch=%{x}\\u003cbr\\u003eLoss=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"1\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"test loss\",\"showlegend\":true,\"x\":[0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,750,760,770,780,790,800,810,820,830,840,850,860,870,880,890,900,910,920,930,940,950,960,970,980,990,1000,1010,1020,1030,1040,1050,1060,1070,1080,1090,1100,1110,1120,1130,1140,1150,1160,1170,1180,1190,1200,1210,1220,1230,1240,1250,1260,1270,1280,1290,1300,1310,1320,1330,1340,1350,1360,1370,1380,1390,1400,1410,1420,1430,1440,1450,1460,1470,1480,1490,1500,1510,1520,1530,1540,1550,1560,1570,1580,1590,1600,1610,1620,1630,1640,1650,1660,1670,1680,1690,1700,1710,1720,1730,1740,1750,1760,1770,1780,1790,1800,1810,1820,1830,1840,1850,1860,1870,1880,1890,1900,1910,1920,1930,1940,1950,1960,1970,1980,1990,2000,2010,2020,2030,2040,2050,2060,2070,2080,2090,2100,2110,2120,2130,2140,2150,2160,2170,2180,2190,2200,2210,2220,2230,2240,2250,2260,2270,2280,2290,2300,2310,2320,2330,2340,2350,2360,2370,2380,2390,2400,2410,2420,2430,2440,2450,2460,2470,2480,2490,2500,2510,2520,2530,2540,2550,2560,2570,2580,2590,2600,2610,2620,2630,2640,2650,2660,2670,2680,2690,2700,2710,2720,2730,2740,2750,2760,2770,2780,2790,2800,2810,2820,2830,2840,2850,2860,2870,2880,2890,2900,2910,2920,2930,2940,2950,2960,2970,2980,2990,3000,3010,3020,3030,3040,3050,3060,3070,3080,3090,3100,3110,3120,3130,3140,3150,3160,3170,3180,3190,3200,3210,3220,3230,3240,3250,3260,3270,3280,3290,3300,3310,3320,3330,3340,3350,3360,3370,3380,3390,3400,3410,3420,3430,3440,3450,3460,3470,3480,3490,3500,3510,3520,3530,3540,3550,3560,3570,3580,3590,3600,3610,3620,3630,3640,3650,3660,3670,3680,3690,3700,3710,3720,3730,3740,3750,3760,3770,3780,3790,3800,3810,3820,3830,3840,3850,3860,3870,3880,3890,3900,3910,3920,3930,3940,3950,3960,3970,3980,3990,4000,4010,4020,4030,4040,4050,4060,4070,4080,4090,4100,4110,4120,4130,4140,4150,4160,4170,4180,4190,4200,4210,4220,4230,4240,4250,4260,4270,4280,4290,4300,4310,4320,4330,4340,4350,4360,4370,4380,4390,4400,4410,4420,4430,4440,4450,4460,4470,4480,4490,4500,4510,4520,4530,4540,4550,4560,4570,4580,4590,4600,4610,4620,4630,4640,4650,4660,4670,4680,4690,4700,4710,4720,4730,4740,4750,4760,4770,4780,4790,4800,4810,4820,4830,4840,4850,4860,4870,4880,4890,4900,4910,4920,4930,4940,4950,4960,4970,4980,4990,5000,5010,5020,5030,5040,5050,5060,5070,5080,5090,5100,5110,5120,5130,5140,5150,5160,5170,5180,5190,5200,5210,5220,5230,5240,5250,5260,5270,5280,5290,5300,5310,5320,5330,5340,5350,5360,5370,5380,5390,5400,5410,5420,5430,5440,5450,5460,5470,5480,5490,5500,5510,5520,5530,5540,5550,5560,5570,5580,5590,5600,5610,5620,5630,5640,5650,5660,5670,5680,5690,5700,5710,5720,5730,5740,5750,5760,5770,5780,5790,5800,5810,5820,5830,5840,5850,5860,5870,5880,5890,5900,5910,5920,5930,5940,5950,5960,5970,5980,5990],\"xaxis\":\"x\",\"y\":[4.7329097173240715,4.753875381466617,4.78211367284997,4.921022742125868,5.163527435338234,5.348903652198737,5.57844766955967,5.847375853607448,6.163107228622158,6.561355086572166,7.046444783697922,7.620792825179323,8.31394337391231,9.157390152365112,10.16436729251672,11.314569721219915,12.642714460172044,14.094858905948819,15.412471857043622,16.396208737176636,16.992532543179575,17.298305042073977,17.43560663058406,17.49288798302793,17.518892948404527,17.542415192741377,17.577676803443502,17.626611470063427,17.689576147234074,17.764472218964762,17.846763264982453,17.933840235251264,18.023474231584522,18.112772279281227,18.20288117042341,18.294318453981045,18.38629368043349,18.47902350259633,18.573201921849083,18.66716131189268,18.7613597846651,18.85629496452026,18.950643403443163,19.04661791180226,19.14402002687052,19.242110700356054,19.341513627275276,19.440285669878957,19.53948701637528,19.638885244663854,19.739023661065293,19.83921459537885,19.939204919995515,20.03898046638595,20.139218136819757,20.24045215713337,20.34131208612003,20.441591737304154,20.5422481844575,20.642720376854168,20.742931550072814,20.841996772268633,20.941883675781636,21.04063954815245,21.137895986104322,21.23593937890969,21.333477541342404,21.431188288109876,21.529484611504557,21.625943043901003,21.72394860083637,21.822118184794547,21.919101803853618,22.01674035886898,22.113226983837446,22.21010111225042,22.307359533921392,22.405527435740606,22.50341789886007,22.60140576627884,22.698993422860333,22.796026748299443,22.894648158239843,22.992593335735855,23.08941673583804,23.18514812501474,23.281008819601496,23.377765960997866,23.4735485685215,23.56914564504921,23.663353431495363,23.756711027828086,23.84891421186806,23.9415880653216,24.03359832468784,24.12579501478986,24.214362308610987,24.30233194704406,24.388864196219703,24.473703048012716,24.557560672747556,24.640107652524936,24.721760091241848,24.801669956130493,24.880065267553857,24.95664912183583,25.030990645580722,25.10372088796973,25.17389542996633,25.24246041957114,25.308529884591835,25.372531993877246,25.433284213793403,25.492691598481354,25.550241785497988,25.604914842262236,25.656249089373365,25.705766011199426,25.75219478143974,25.794816164899736,25.834290891953167,25.871261410340257,25.90394435494244,25.933733789986814,25.961706554502957,25.985359030537996,26.006943610485244,26.026250150445282,26.041948640688123,26.054113577944314,26.0634989770828,26.069316900152575,26.072019808553417,26.072224577103196,26.069033203319655,26.063263627658184,26.054810652946156,26.043140747334586,26.02914245525532,26.011195724686534,25.9916080753518,25.970697851751154,25.946386688610048,25.919647890945924,25.890910915037857,25.859816987761292,25.827489034265028,25.792466037287827,25.755935569838382,25.719758710806083,25.68072763690791,25.641339649542243,25.600106535495172,25.559390689579118,25.516237572551287,25.47404543716846,25.430460275788317,25.387139023731777,25.342447160349806,25.2980367697986,25.253437697493794,25.207927603165537,25.16310271637548,25.117926513865697,25.07321038179207,25.029131332503464,24.984627492095658,24.939233538575195,24.894880316659677,24.85013763885022,24.805225982037864,24.762900379249576,24.7191087675631,24.675602722188202,24.632109706163956,24.588439139773115,24.544390192401114,24.501701134705286,24.458615917947032,24.41719493728758,24.373445463116333,24.33151715854189,24.28866149851207,24.244652400105075,24.20099507449544,24.158797833092525,24.116266863126285,24.07310433461284,24.031957414257292,23.988347759188045,23.94727065582618,23.905851762580813,23.862885919365667,23.8201316797606,23.777652626417368,23.73672996952294,23.693896366623807,23.651022580798003,23.608812617036016,23.566331612605776,23.523449735639968,23.481936101647985,23.438316502127606,23.396276370867835,23.35533612073931,23.31232903348593,23.270695439351204,23.229633204725182,23.185761738398465,23.14486144832201,23.102814158410467,23.059961369346922,23.016752575633227,22.974514214657795,22.932579797216185,22.888746546765727,22.846818333937364,22.80457214351585,22.76288115830891,22.721217928598456,22.6775350474546,22.634964150436883,22.591541860479126,22.54854472443692,22.50571050565424,22.463604739172283,22.421399908649352,22.378294205444497,22.33513619984281,22.292653812392413,22.251076452513896,22.207344499247643,22.16471315508175,22.122258148296204,22.079946271504923,22.03718566870282,21.994964374798318,21.951840096614642,21.908248272502117,21.86510504345388,21.822208745997052,21.779613793887197,21.737621686156835,21.69511183554124,21.652006142850396,21.609269167858436,21.564952633686644,21.521858787088195,21.478577929862702,21.435977517352683,21.391935672702573,21.349123649797676,21.304417420981853,21.260939134118175,21.21798132542855,21.17448423271258,21.12973185065872,21.086860175557092,21.042686093906074,20.99826205709847,20.95542859250281,20.91139766684707,20.867810325252247,20.824764513015452,20.77988017594909,20.735720860545108,20.692338244351394,20.646900857985674,20.601425929597426,20.556667244281666,20.510925061747418,20.465026738932465,20.420517971566927,20.37401336898008,20.327978334345655,20.28219979096657,20.235753071374287,20.189286067891835,20.143636805228724,20.09654459929813,20.048867267423383,20.002044299906455,19.95641598234552,19.909333710868932,19.862079580326217,19.81499073234646,19.767697442204593,19.721254301667653,19.67290362398835,19.624459530412068,19.57687162832721,19.52940834507612,19.480856074813623,19.43162128867979,19.384045559079254,19.33514308739824,19.286333181207425,19.23843332656364,19.189075444646537,19.139059445847995,19.089361553887702,19.039190813289675,18.988664282589998,18.9384498961379,18.887630139660825,18.83760288331292,18.78466002379127,18.73274774190142,18.68009573661135,18.62742528224433,18.575665456574146,18.522689300255227,18.470849168738223,18.41798904916634,18.36456883590371,18.311933194386647,18.25731992014437,18.202887971576907,18.148991889887508,18.093817277796237,18.038833966104736,17.984951432387312,17.93030706563742,17.875234707458812,17.82064746186744,17.76612535195105,17.710531431008665,17.65512868974348,17.600444986171084,17.543893655768613,17.488204807244642,17.433315345176116,17.375571777394583,17.319123921149775,17.260552880787344,17.203024774379077,17.143259246611233,17.083597881541156,17.02525090777738,16.965429224102845,16.90530227676976,16.84626489463187,16.787180402366047,16.726493194823945,16.66620403503463,16.606631428041133,16.546034377173623,16.486313200167345,16.426352899393777,16.365409066341353,16.305619433956515,16.246503712415468,16.185708806572617,16.12611720302711,16.066667008969265,16.004861316270024,15.944434325247599,15.883478608490938,15.821874486646413,15.759228550994468,15.696438130819772,15.63356687183981,15.57110716853057,15.508031934295417,15.445068948709718,15.382094709822104,15.319021190721433,15.256736837905429,15.193402138863028,15.129624676486218,15.066265374941247,15.002143451094865,14.93829331043954,14.87386255859523,14.809545427613232,14.744137392237095,14.679099082758457,14.613553185506452,14.545845221209683,14.479241104113882,14.411478824632075,14.344149190215965,14.27596128780817,14.209354208115702,14.141098271671016,14.07334487500624,14.00529629764556,13.937632036754133,13.870054027506148,13.802921712489418,13.734924670678597,13.665953016211029,13.597513264866716,13.529375457844969,13.460649884922537,13.390064652128267,13.319268330969033,13.249196424014777,13.179010224600763,13.10820258836456,13.038228662514992,12.966692588348964,12.895280870567444,12.823316102064247,12.75082839330326,12.678364966360288,12.605877572498843,12.533164045233287,12.459880422255555,12.387510214782285,12.313631612790726,12.24030708165211,12.165627113874466,12.091070508011503,12.016198239029174,11.940613568016227,11.865154146220995,11.789120651305426,11.712935184843943,11.637053876544028,11.560907852319419,11.48459378504058,11.408427803257498,11.331382339645934,11.254164117058046,11.177827744613193,11.100927249167492,11.022948181665003,10.945368020885407,10.867194507602708,10.789426359881087,10.711150105146187,10.63212291169269,10.553058928339047,10.474098697493362,10.394291348232283,10.31492402407268,10.23530262093062,10.155579078440306,10.07548436656121,9.994715800444025,9.913762222222806,9.833110975876203,9.752120451521709,9.671683715472602,9.59118092822852,9.50988703686123,9.427457887340067,9.345309326417224,9.2627221602862,9.179570634060036,9.096037152186197,9.012141045309015,8.927380452360834,8.842524401484598,8.75734558743952,8.671685831114859,8.586049736705046,8.5006388849372,8.414808465372621,8.328517560084812,8.24113096483342,8.152642764297163,8.063810789369706,7.975383898683172,7.886734097574817,7.797610391105029,7.708463817477563,7.619063903365551,7.529223250254518,7.439084547695933,7.348656944571916,7.2573707197809565,7.16668342158775,7.075892752110391,6.985986836713814,6.896071440040165,6.805650807092756,6.715107020950644,6.623559762765968,6.531875282750215,6.439925657779251,6.347834847919757,6.254874032372068,6.161586201332504,6.068179855942639,5.9741880312832425,5.879692645072833,5.7845824049937615,5.689621620397858,5.594872460773384,5.498947283794036,5.402875701874843,5.306448630443953,5.209541636844144,5.11180771020994,5.013680093458725,4.9149591130968915,4.81622389956501,4.717812498374735,4.619358807304442,4.5211538246984135,4.423986722939654,4.32631748496671,4.229019798317941,4.131892790882971,4.035535763830395,3.938680148831451,3.8417636311694743,3.745118186419048,3.6487316116678845,3.5523523422555563,3.4564585661574787,3.3606790719958153,3.2648106003758466,3.169493592930635,3.074961486177853,2.981385199972574,2.888201682416786,2.796302361502058,2.705534350830231,2.6162086115402787,2.5276257003337146,2.439995034918322,2.353691522883055,2.2681530751727035,2.1841819833094576,2.101890359703803,2.0212727087440387,1.9425645194086558,1.865577240966687,1.790062989370644,1.7160905859615159,1.644372474995672,1.574529009016788,1.5061129562353268,1.4391889890022913,1.3746697309659772,1.3122702131557087,1.2516995457673603,1.1932664410850389,1.1364633554318728,1.081397427450604,1.0284266578880092,0.9774635084924367,0.9281038325632166,0.8804029576890973,0.8345813141960983,0.7906022220593641,0.7482105058122854,0.7076197194504723,0.6684869510618825,0.6310505111173217,0.5957657104192594,0.5618423165347808,0.5295617097268833,0.4990140262550668,0.469687241778896,0.44186385884044455,0.4155831464171786,0.39045387655830394,0.36651454354103397,0.3439582562717433,0.3226826096362006,0.302594650885684,0.2836904436477162,0.2659470007456084,0.24915498896194507,0.23348604397012748,0.21878708418530946,0.20504480786538448,0.19191582531005533,0.17960701226773637,0.1679402585697188,0.15695766538412595,0.14671589418932762,0.13712007864091955,0.1281372555489311,0.11981596747246881,0.11181709948356872,0.10446795065953153,0.09749116208164486,0.09096298169362069,0.08489096355256912,0.07919628844610013,0.07380203262014995,0.06889272219944548,0.0642620695210482,0.05994765778370077,0.055981751555403914,0.05225460647245936,0.04873022008139913,0.045468357627903945,0.04240388583959906,0.039495938707748555,0.03681799324099815,0.03432538122967427,0.03202796032069404,0.02991629249502744,0.02795481216180665],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15,\"font\":{\"size\":20}},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15,\"font\":{\"size\":20}},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05,\"font\":{\"size\":30}},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss\"}},\"legend\":{\"title\":{\"text\":\"Color\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Training Curve for Modular Arithmetic - 35% Addition\"},\"updatemenus\":[{\"active\":-1,\"buttons\":[{\"args\":[{\"xaxis.type\":\"log\"}],\"args2\":[{\"xaxis.type\":\"linear\"}],\"label\":\"Log x-axis\",\"method\":\"relayout\"}],\"type\":\"buttons\",\"x\":-0.1,\"y\":1.0},{\"active\":-1,\"buttons\":[{\"args\":[{\"yaxis.type\":\"log\"}],\"args2\":[{\"yaxis.type\":\"linear\"}],\"label\":\"Log y-axis\",\"method\":\"relayout\"}],\"type\":\"buttons\",\"x\":-0.1,\"y\":0.85}]},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('dc31b3f5-4823-4963-84ba-94c0a3bb6255');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                });            </script>        </div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>                            <div id=\"8d84a2b7-cba4-45c6-9af4-04248e513269\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8d84a2b7-cba4-45c6-9af4-04248e513269\")) {                    Plotly.newPlot(                        \"8d84a2b7-cba4-45c6-9af4-04248e513269\",                        [{\"hovertemplate\":\"Color=train\\u003cbr\\u003eEpoch=%{x}\\u003cbr\\u003eRolling Avg Loss=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"train\",\"showlegend\":true,\"x\":[0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,750,760,770,780,790,800,810,820,830,840,850,860,870,880,890,900,910,920,930,940,950,960,970,980,990,1000,1010,1020,1030,1040,1050,1060,1070,1080,1090,1100,1110,1120,1130,1140,1150,1160,1170,1180,1190,1200,1210,1220,1230,1240,1250,1260,1270,1280,1290,1300,1310,1320,1330,1340,1350,1360,1370,1380,1390,1400,1410,1420,1430,1440,1450,1460,1470,1480,1490,1500,1510,1520,1530,1540,1550,1560,1570,1580,1590,1600,1610,1620,1630,1640,1650,1660,1670,1680,1690,1700,1710,1720,1730,1740,1750,1760,1770,1780,1790,1800,1810,1820,1830,1840,1850,1860,1870,1880,1890,1900,1910,1920,1930,1940,1950,1960,1970,1980,1990,2000,2010,2020,2030,2040,2050,2060,2070,2080,2090,2100,2110,2120,2130,2140,2150,2160,2170,2180,2190,2200,2210,2220,2230,2240,2250,2260,2270,2280,2290,2300,2310,2320,2330,2340,2350,2360,2370,2380,2390,2400,2410,2420,2430,2440,2450,2460,2470,2480,2490,2500,2510,2520,2530,2540,2550,2560,2570,2580,2590,2600,2610,2620,2630,2640,2650,2660,2670,2680,2690,2700,2710,2720,2730,2740,2750,2760,2770,2780,2790,2800,2810,2820,2830,2840,2850,2860,2870,2880,2890,2900,2910,2920,2930,2940,2950,2960,2970,2980,2990,3000,3010,3020,3030,3040,3050,3060,3070,3080,3090,3100,3110,3120,3130,3140,3150,3160,3170,3180,3190,3200,3210,3220,3230,3240,3250,3260,3270,3280,3290,3300,3310,3320,3330,3340,3350,3360,3370,3380,3390,3400,3410,3420,3430,3440,3450,3460,3470,3480,3490,3500,3510,3520,3530,3540,3550,3560,3570,3580,3590,3600,3610,3620,3630,3640,3650,3660,3670,3680,3690,3700,3710,3720,3730,3740,3750,3760,3770,3780,3790,3800,3810,3820,3830,3840,3850,3860,3870,3880,3890,3900,3910,3920,3930,3940,3950,3960,3970,3980,3990,4000,4010,4020,4030,4040,4050,4060,4070,4080,4090,4100,4110,4120,4130,4140,4150,4160,4170,4180,4190,4200,4210,4220,4230,4240,4250,4260,4270,4280,4290,4300,4310,4320,4330,4340,4350,4360,4370,4380,4390,4400,4410,4420,4430,4440,4450,4460,4470,4480,4490,4500,4510,4520,4530,4540,4550,4560,4570,4580,4590,4600,4610,4620,4630,4640,4650,4660,4670,4680,4690,4700,4710,4720,4730,4740,4750,4760,4770,4780,4790,4800,4810,4820,4830,4840,4850,4860,4870,4880,4890,4900,4910,4920,4930,4940,4950,4960,4970,4980,4990,5000,5010,5020,5030,5040,5050,5060,5070,5080,5090,5100,5110,5120,5130,5140,5150,5160,5170,5180,5190,5200,5210,5220,5230,5240,5250,5260,5270,5280,5290,5300,5310,5320,5330,5340,5350,5360,5370,5380,5390,5400,5410,5420,5430,5440,5450,5460,5470,5480,5490,5500,5510,5520,5530,5540,5550,5560,5570,5580,5590,5600,5610,5620,5630,5640,5650,5660,5670,5680,5690,5700,5710,5720,5730,5740,5750,5760,5770,5780,5790,5800,5810,5820,5830,5840,5850,5860,5870,5880,5890,5900,5910,5920,5930,5940,5950,5960,5970,5980,5990],\"xaxis\":\"x\",\"y\":[4.734745349574246,4.718392292090453,4.706956786492355,4.686263698390323,4.65400786908605,4.614399958807485,4.568578325605684,4.514750856406769,4.449177502778149,4.36692380201158,4.263496842961739,4.137273809388803,3.99022939918354,3.826398943166069,3.651747988637891,3.471559714209964,3.2914137265327637,3.1169469140623476,2.9532155285354245,2.802845847196765,2.6555601305245222,2.421547141343161,2.1883067828739953,1.9574441690832445,1.7308831067301216,1.5093115392128096,1.2934975103552273,1.085016465297507,0.8865900707207593,0.7022318398893402,0.5367195346297313,0.39413666686447507,0.27671471020960936,0.18494007396029155,0.11713069392023033,0.07092593015827835,0.0426694061204472,0.027539252967700576,0.020111259948122528,0.01627024281809955,0.013947967572596216,0.012296473409677577,0.010973080194132893,0.00983418098127413,0.00881890394656322,0.007901520457779664,0.007070996630136101,0.006321323695352965,0.005647535759903674,0.005044157025377247,0.004505287400791951,0.004024848925893131,0.0035967827013212534,0.0032153619790472655,0.002875402160099201,0.0025723071805093594,0.0023019145667596535,0.0020606093829589455,0.0018451748641253973,0.001652702439650667,0.0014806615031774154,0.0013268248734114513,0.001189221253074829,0.0010660689292303987,0.0009558135812939285,0.0008570970552972874,0.0007686681156211815,0.0006894810514036539,0.0006185493326285961,0.0005550048070049047,0.0004980533919414664,0.00044699830865340204,0.00040122100465634497,0.0003601728573844759,0.0003233604623281152,0.00029035001944514063,0.00026074504712552353,0.0002341913200598497,0.0002103719304333317,0.00018900728361597518,0.0001698406072580182,0.0001526439049178084,0.00013721369239210764,0.00012336133269123898,0.0001109223149481922,9.975146435015251e-05,8.971988598352885e-05,8.071074850914833e-05,7.261827850504792e-05,6.534847992981941e-05,5.8817332451860836e-05,5.2949312570429904e-05,4.76762758089787e-05,4.293705420894129e-05,3.8677076270700065e-05,3.4847607756042006e-05,3.1404355520480344e-05,2.830791147990699e-05,2.552338771578329e-05,2.3018651071936526e-05,2.076531620163967e-05,1.873837930521057e-05,1.6914966592555512e-05,1.527426052256152e-05,1.3797571847242272e-05,1.2468522640047846e-05,1.1272307740327356e-05,1.0195466197535442e-05,9.226175744446151e-06,8.353771714858286e-06,7.568198914522015e-06,6.860703732099678e-06,6.223330336322305e-06,5.649106070122038e-06,5.131704630427772e-06,4.665356788518663e-06,4.245201072449787e-06,3.8666462146707515e-06,3.5254765524430313e-06,3.2179398903373175e-06,2.9407665417780764e-06,2.690884957281311e-06,2.465502230385287e-06,2.262151199601368e-06,2.078639997279044e-06,1.9130142247168746e-06,1.7634633794132718e-06,1.6284230448901347e-06,1.5064618394349131e-06,1.3963063926991734e-06,1.2967486281884797e-06,1.2067922818578573e-06,1.1254743601693139e-06,1.0519408720601646e-06,9.854307006742597e-07,9.252696894537586e-07,8.708553702155027e-07,8.216293932136378e-07,7.770974576725323e-07,7.368195451706691e-07,7.003925169268488e-07,6.674493364141213e-07,6.376483351343755e-07,6.107073267012914e-07,5.863528259591866e-07,5.643512388408117e-07,5.444821252006421e-07,5.265611581001645e-07,5.104011463408906e-07,4.958499686574787e-07,4.82753954364891e-07,4.7099247606429297e-07,4.6042732889994524e-07,4.5096413113770904e-07,4.424913909593266e-07,4.3492930778880284e-07,4.281773002721434e-07,4.221698976604556e-07,4.1682748266396433e-07,4.12087697059294e-07,4.0790062334483233e-07,4.0419975767487795e-07,4.0094718414389867e-07,3.980837269381353e-07,3.9557582512943203e-07,3.9337259909456465e-07,3.914488440428751e-07,3.897612592419117e-07,3.88290125579162e-07,3.870050403644172e-07,3.858827098785718e-07,3.848977238381311e-07,3.840444191284917e-07,3.8328835709875293e-07,3.826365863439217e-07,3.820563955957887e-07,3.815568485234046e-07,3.8110970067227213e-07,3.8072234697038087e-07,3.8036956359623085e-07,3.800465850818411e-07,3.7975635279308834e-07,3.794821150535393e-07,3.7923283569906693e-07,3.789958950083008e-07,3.787791402367683e-07,3.7857664100776214e-07,3.783825435746602e-07,3.7819498281642795e-07,3.780145885680139e-07,3.77841065946869e-07,3.776734287760024e-07,3.775071831097894e-07,3.773442227364881e-07,3.771762675111747e-07,3.770101876720269e-07,3.768395814121325e-07,3.766719484713408e-07,3.764961135461889e-07,3.763299673994599e-07,3.7615819454127736e-07,3.759841059034795e-07,3.7581794313098686e-07,3.756413768833344e-07,3.754703881874692e-07,3.7529949464954075e-07,3.751200439143097e-07,3.7494236307030235e-07,3.747645675472495e-07,3.745831573950363e-07,3.744061218551228e-07,3.742261168536125e-07,3.7404324696422313e-07,3.7386090126121244e-07,3.7367375450988334e-07,3.7348970120869496e-07,3.733010621995759e-07,3.731124933787032e-07,3.729316432386035e-07,3.7273978224209477e-07,3.7255496341904476e-07,3.723706465662363e-07,3.721780205244736e-07,3.719891601395289e-07,3.717970771520317e-07,3.7160158507373644e-07,3.7140451863105536e-07,3.7120964388735513e-07,3.7100916751542466e-07,3.7080441515157383e-07,3.7060374787268564e-07,3.7039713035046676e-07,3.701944127437444e-07,3.6999099064216954e-07,3.697904972097357e-07,3.695857166806998e-07,3.693810518913691e-07,3.691754251363966e-07,3.6896173854370324e-07,3.6875420388630416e-07,3.685428086568685e-07,3.6832981045649963e-07,3.6812180195391264e-07,3.679123154743001e-07,3.6770554822081286e-07,3.674926956688386e-07,3.6729829376100676e-07,3.670986632886167e-07,3.669027141701554e-07,3.667118564719191e-07,3.6651037795593985e-07,3.663142978842818e-07,3.6611092678976395e-07,3.659085142373807e-07,3.6570810463808677e-07,3.655060143117083e-07,3.6530209299376567e-07,3.650988125609645e-07,3.648967510941221e-07,3.6469376066990443e-07,3.644958122643869e-07,3.6429804151081956e-07,3.640949298972001e-07,3.6389646898175806e-07,3.636919428202519e-07,3.634892426600876e-07,3.6327361388434583e-07,3.6306328613070325e-07,3.628462997358563e-07,3.62631037393636e-07,3.6241610288183306e-07,3.621993654849246e-07,3.619843602827837e-07,3.6177024892210663e-07,3.6155224068832976e-07,3.6133466832213744e-07,3.6111850688000696e-07,3.6089704773903957e-07,3.6067668550907513e-07,3.6045561047624565e-07,3.60226027300308e-07,3.59997820034375e-07,3.597711262982616e-07,3.595410438824976e-07,3.593088683712919e-07,3.5907860615491844e-07,3.5884602396442466e-07,3.586082310593825e-07,3.5837755946232053e-07,3.5814299904144773e-07,3.5790993966010045e-07,3.5767615655459825e-07,3.5744185049275786e-07,3.572059497962835e-07,3.5696970615696377e-07,3.5673451932065457e-07,3.5649856910882467e-07,3.562661646448772e-07,3.5603000488221304e-07,3.557903459569256e-07,3.555518860061823e-07,3.553121578674501e-07,3.550700975798752e-07,3.548300921969334e-07,3.5458727303992104e-07,3.543489844418113e-07,3.5411057346135264e-07,3.5387304074666573e-07,3.5362760890059405e-07,3.533803044391385e-07,3.5313447215466076e-07,3.5289045718135515e-07,3.526472376655747e-07,3.524011362293002e-07,3.521484861795519e-07,3.518976853363329e-07,3.5164032385062754e-07,3.5138230531857556e-07,3.511270984067458e-07,3.508765427542246e-07,3.5062719746832267e-07,3.503787966765484e-07,3.501288575668544e-07,3.498765070445173e-07,3.49626543307394e-07,3.4936723008160487e-07,3.4911024990957456e-07,3.488517019545233e-07,3.48595697383298e-07,3.483370108331952e-07,3.4807171839869375e-07,3.4780489288999355e-07,3.4753306931610566e-07,3.4726238984575227e-07,3.469934441086468e-07,3.4672134245766124e-07,3.464562929027528e-07,3.4618566031613613e-07,3.459143617594643e-07,3.456385070306062e-07,3.453597719969334e-07,3.450804224255026e-07,3.44800440267645e-07,3.445189898183001e-07,3.4423914609533545e-07,3.4395781297637146e-07,3.4367380839376444e-07,3.4339689853895975e-07,3.431174542968409e-07,3.428404434658341e-07,3.425681829974107e-07,3.422878208002178e-07,3.420108822544246e-07,3.4173279832702874e-07,3.414565293439106e-07,3.4117931180366844e-07,3.4089457714625205e-07,3.4061217122817926e-07,3.403258984953215e-07,3.4003984785696186e-07,3.397483741065824e-07,3.394589962211747e-07,3.3916707763084766e-07,3.388786580683591e-07,3.3859181024674934e-07,3.3830685237829675e-07,3.380200271525846e-07,3.3772507727341e-07,3.3743201256102207e-07,3.3713565159318814e-07,3.368400959951685e-07,3.365502816097656e-07,3.362596554341032e-07,3.359684668696007e-07,3.3567148210380613e-07,3.353730183692295e-07,3.3507575184146646e-07,3.3477398559175006e-07,3.344690314585651e-07,3.341591064566477e-07,3.338542223351417e-07,3.335439063645289e-07,3.3323423621435745e-07,3.3291774347643963e-07,3.3259576032936376e-07,3.3227076562124003e-07,3.3194290116776396e-07,3.316152233117061e-07,3.312874586470792e-07,3.309589203552023e-07,3.306276762374519e-07,3.3029000746981974e-07,3.29945282803233e-07,3.2960093392085715e-07,3.292592994629223e-07,3.2891341270457666e-07,3.285697613472891e-07,3.2822638319454084e-07,3.27884880717737e-07,3.2754851807391613e-07,3.2721317560840826e-07,3.26873086087589e-07,3.2653365265923067e-07,3.2619604225710657e-07,3.2586045818541705e-07,3.255229190698067e-07,3.251844495763863e-07,3.2484429689688233e-07,3.244987743421345e-07,3.2415464793142673e-07,3.2380507674993366e-07,3.2345788891931077e-07,3.23107574726503e-07,3.2275186313225163e-07,3.2239491870389297e-07,3.2203838514602844e-07,3.216816584201097e-07,3.2132410588459715e-07,3.2096316314502736e-07,3.2060012706712374e-07,3.20231781548429e-07,3.198611547190118e-07,3.1948540539788665e-07,3.1910366247422933e-07,3.187139405492152e-07,3.183234943467819e-07,3.179292872044339e-07,3.1753258569645947e-07,3.171326045461757e-07,3.1673007605604337e-07,3.1632873351232574e-07,3.1592192230365535e-07,3.15518163785215e-07,3.1511025293509016e-07,3.146980441923819e-07,3.1428238568351536e-07,3.138589136325846e-07,3.13434339410552e-07,3.130097619585945e-07,3.125811763126872e-07,3.1214694376683034e-07,3.117084756828261e-07,3.112667602226309e-07,3.108237837407355e-07,3.1038216615594885e-07,3.0994007379798935e-07,3.094985021384923e-07,3.0905207569671e-07,3.0860423903101794e-07,3.0815125648325926e-07,3.0769155740273553e-07,3.072285895315104e-07,3.0676019426874724e-07,3.0628407730951524e-07,3.058025885281865e-07,3.053218519288205e-07,3.048406572692171e-07,3.043585003461093e-07,3.0387131188029124e-07,3.033765835249762e-07,3.028778697549701e-07,3.0237229117921637e-07,3.0186078763963034e-07,3.0134144147879534e-07,3.0081365233221725e-07,3.002797225763009e-07,2.9973828552534353e-07,2.9919458696499794e-07,2.986403011803487e-07,2.9807610419142096e-07,2.975050156530606e-07,2.969286550826641e-07,2.96348097817884e-07,2.957683305827664e-07,2.9518894573391447e-07,2.946030357286918e-07,2.940104589191752e-07,2.93409438295128e-07,2.927996550191646e-07,2.9218458902578584e-07,2.915626998061059e-07,2.9093905802754457e-07,2.903102420754493e-07,2.896767756080689e-07,2.8903906249597436e-07,2.883944352857286e-07,2.8774056611180697e-07,2.8707013205211793e-07,2.8638886410995766e-07,2.8570085971365206e-07,2.850070146233683e-07,2.843014916347218e-07,2.8358125039614734e-07,2.828470970138883e-07,2.820959000051276e-07,2.8132626721019043e-07,2.805408453641338e-07,2.7973935028622573e-07,2.7892545520703433e-07,2.7809747446674367e-07,2.772554796422154e-07,2.7640086737707433e-07,2.755391400788092e-07,2.746685617270132e-07,2.7378763464372986e-07,2.728962896394406e-07,2.719941770962806e-07,2.710812868543176e-07,2.701573219055568e-07,2.69215218373167e-07,2.682583227215513e-07,2.672900546747281e-07,2.663064770678235e-07,2.6530752948979587e-07,2.6429593885831435e-07,2.63275858462068e-07,2.622484003442577e-07,2.612139935727421e-07,2.6016952021160597e-07,2.591169862516893e-07,2.5805676563986345e-07,2.569888163901627e-07,2.5590522430203216e-07,2.5480702693883115e-07,2.536928769922141e-07,2.525675270145681e-07,2.514315951096446e-07,2.502948898302492e-07,2.4916346882252015e-07,2.4803577499202377e-07,2.469122375762229e-07,2.4579273968806695e-07,2.4468042077207676e-07,2.435748787919187e-07,2.4247426306266427e-07,2.4137470099123866e-07,2.4027024821345873e-07,2.3916176324025e-07,2.3806009689297242e-07,2.369596479507066e-07,2.3586794456833416e-07,2.3478197474514187e-07,2.3370587797767398e-07,2.326369057383514e-07,2.315745284560608e-07,2.3051322694357093e-07,2.2945477935109402e-07,2.2840268133740201e-07,2.2735601326232902e-07,2.263154120157101e-07,2.2527059743415529e-07,2.242221977190676e-07,2.2317339244649963e-07,2.2212654803694892e-07,2.210818379236516e-07,2.2004470022261514e-07,2.1901977109863386e-07,2.1800374593766695e-07,2.1698278532827312e-07,2.159586777668214e-07,2.1493000291789643e-07,2.1389703905128529e-07,2.1285575310091578e-07,2.118163817761436e-07,2.1078257857838858e-07,2.097555447530447e-07,2.087367676540062e-07,2.077165486154444e-07,2.066941230227107e-07,2.0567385980881293e-07,2.0466254963188683e-07,2.036548481219688e-07,2.0264720302527202e-07,2.0163837352386658e-07,2.0063037809972586e-07,1.996248034234477e-07,1.9862197297956972e-07,1.9762646962556497e-07,1.966410351556587e-07,1.956681232716913e-07,1.947029026057879e-07,1.9374608830630185e-07,1.9280439113022533e-07,1.9186959203709034e-07,1.909425185397652e-07,1.9001721712954132e-07,1.8909703010912256e-07,1.8818662736505861e-07,1.8728072035838857e-07,1.8638220997917527e-07,1.854866962632703e-07,1.8460259926968733e-07,1.8373819075750165e-07,1.8289579156780952e-07,1.820696463782671e-07,1.812550479816943e-07,1.8045153628096913e-07,1.7965492851639695e-07,1.78869811958691e-07,1.7809484502335233e-07],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"Color=test\\u003cbr\\u003eEpoch=%{x}\\u003cbr\\u003eRolling Avg Loss=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"1\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"test\",\"showlegend\":true,\"x\":[0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,350,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,590,600,610,620,630,640,650,660,670,680,690,700,710,720,730,740,750,760,770,780,790,800,810,820,830,840,850,860,870,880,890,900,910,920,930,940,950,960,970,980,990,1000,1010,1020,1030,1040,1050,1060,1070,1080,1090,1100,1110,1120,1130,1140,1150,1160,1170,1180,1190,1200,1210,1220,1230,1240,1250,1260,1270,1280,1290,1300,1310,1320,1330,1340,1350,1360,1370,1380,1390,1400,1410,1420,1430,1440,1450,1460,1470,1480,1490,1500,1510,1520,1530,1540,1550,1560,1570,1580,1590,1600,1610,1620,1630,1640,1650,1660,1670,1680,1690,1700,1710,1720,1730,1740,1750,1760,1770,1780,1790,1800,1810,1820,1830,1840,1850,1860,1870,1880,1890,1900,1910,1920,1930,1940,1950,1960,1970,1980,1990,2000,2010,2020,2030,2040,2050,2060,2070,2080,2090,2100,2110,2120,2130,2140,2150,2160,2170,2180,2190,2200,2210,2220,2230,2240,2250,2260,2270,2280,2290,2300,2310,2320,2330,2340,2350,2360,2370,2380,2390,2400,2410,2420,2430,2440,2450,2460,2470,2480,2490,2500,2510,2520,2530,2540,2550,2560,2570,2580,2590,2600,2610,2620,2630,2640,2650,2660,2670,2680,2690,2700,2710,2720,2730,2740,2750,2760,2770,2780,2790,2800,2810,2820,2830,2840,2850,2860,2870,2880,2890,2900,2910,2920,2930,2940,2950,2960,2970,2980,2990,3000,3010,3020,3030,3040,3050,3060,3070,3080,3090,3100,3110,3120,3130,3140,3150,3160,3170,3180,3190,3200,3210,3220,3230,3240,3250,3260,3270,3280,3290,3300,3310,3320,3330,3340,3350,3360,3370,3380,3390,3400,3410,3420,3430,3440,3450,3460,3470,3480,3490,3500,3510,3520,3530,3540,3550,3560,3570,3580,3590,3600,3610,3620,3630,3640,3650,3660,3670,3680,3690,3700,3710,3720,3730,3740,3750,3760,3770,3780,3790,3800,3810,3820,3830,3840,3850,3860,3870,3880,3890,3900,3910,3920,3930,3940,3950,3960,3970,3980,3990,4000,4010,4020,4030,4040,4050,4060,4070,4080,4090,4100,4110,4120,4130,4140,4150,4160,4170,4180,4190,4200,4210,4220,4230,4240,4250,4260,4270,4280,4290,4300,4310,4320,4330,4340,4350,4360,4370,4380,4390,4400,4410,4420,4430,4440,4450,4460,4470,4480,4490,4500,4510,4520,4530,4540,4550,4560,4570,4580,4590,4600,4610,4620,4630,4640,4650,4660,4670,4680,4690,4700,4710,4720,4730,4740,4750,4760,4770,4780,4790,4800,4810,4820,4830,4840,4850,4860,4870,4880,4890,4900,4910,4920,4930,4940,4950,4960,4970,4980,4990,5000,5010,5020,5030,5040,5050,5060,5070,5080,5090,5100,5110,5120,5130,5140,5150,5160,5170,5180,5190,5200,5210,5220,5230,5240,5250,5260,5270,5280,5290,5300,5310,5320,5330,5340,5350,5360,5370,5380,5390,5400,5410,5420,5430,5440,5450,5460,5470,5480,5490,5500,5510,5520,5530,5540,5550,5560,5570,5580,5590,5600,5610,5620,5630,5640,5650,5660,5670,5680,5690,5700,5710,5720,5730,5740,5750,5760,5770,5780,5790,5800,5810,5820,5830,5840,5850,5860,5870,5880,5890,5900,5910,5920,5930,5940,5950,5960,5970,5980,5990],\"xaxis\":\"x\",\"y\":[4.7329097173240715,4.744724231445761,4.755151443568761,4.783069257533352,4.850478634372553,4.931779274185927,5.02011725613194,5.119164100666788,5.229991441426496,5.355891787340446,5.500747322178977,5.667775443514282,5.859566913200579,6.0815374997228195,6.338176635720709,6.632685534614212,6.968135545597856,7.3465147566418425,7.760476595840692,8.191217085739506,8.636608846410251,9.25833012229602,9.889161876375434,10.52065936366845,11.143118506190886,11.756433471011622,12.360905019337306,12.95489075029715,13.537052593910786,14.104751727805949,14.654253845563563,15.181238154008295,15.680959091075044,16.146209044712837,16.569308933028758,16.942705968008852,17.258269541418144,17.508212810597847,17.69278730295931,17.824654411843174,17.92293822495276,18.00459746159687,18.080854191285468,18.157447037211185,18.237015277825776,18.320332368403502,18.407154569419266,18.496801042044925,18.588532234901294,18.68174377787906,18.775992415069755,18.87097103167573,18.966530124042166,19.062600619343446,19.159212127216776,19.256309867152446,19.35386325522553,19.451834626924782,19.55013156662179,19.648752507378923,19.747704929883145,19.84690540220165,19.946354233453526,20.04599235407284,20.145698704424753,20.245403355123514,20.34504290518299,20.444599959309617,20.544118261712587,20.643548229539647,20.742853420640127,20.842042916542276,20.941124939217097,21.040086205299584,21.13885291481287,21.237428928261625,21.33582511070049,21.434058216481265,21.532201101131935,21.63018959070999,21.728045310651797,21.825796000591673,21.923435072496567,22.021034609807785,22.118612077405892,22.216129229950816,22.31356571754718,22.41091898213697,22.50819520252216,22.605384733487455,22.70242767191581,22.79926145830504,22.89585543150047,22.99221506399761,23.088350537036636,23.184225242031108,23.279759567348336,23.374827699816873,23.46937462329341,23.56331090403304,23.65655928115895,23.749107547686823,23.84085777486597,23.931731415759028,24.02171224967888,24.110723633569247,24.198700615046008,24.285551545029886,24.371153208969936,24.45544982598764,24.538371031957467,24.619840812691265,24.699771380478023,24.778084551793526,24.854701519112886,24.92951686480455,25.00248235918399,25.073547761864244,25.142603549119777,25.209606374142858,25.274435869684257,25.337006845687238,25.39722233321894,25.454972632838306,25.51019039376903,25.562826808234817,25.612823973763387,25.660139229305518,25.704756897789352,25.74658505184384,25.785609251966747,25.821769841785404,25.855012344204237,25.885309967216898,25.912597696602294,25.936876290427364,25.958141000162556,25.976382047403085,25.991592835602205,26.003787521044945,26.0130024171877,26.019272004545336,26.02265823658151,26.02321712123904,26.020936544003984,26.015880429891595,26.008119381614144,25.997669196623313,25.984572189005444,25.968942473918883,25.950874685339432,25.93047133971166,25.907853473184804,25.88309942754536,25.856342971334065,25.827677077256123,25.797207696698447,25.76508193600863,25.731419225713843,25.69633161021562,25.659949380113172,25.622337176436325,25.583634716540566,25.54394627147665,25.503413137322212,25.462153465158107,25.420246663165408,25.377806169889762,25.33490562030122,25.291617488097216,25.24799081750918,25.204149331717044,25.160130064461523,25.115995721072046,25.071783937933915,25.027525408506868,24.98326351626812,24.939020213340324,24.894823941313607,24.850727769591387,24.806740081141957,24.762888446586192,24.719119594391472,24.67543295234772,24.63181382267882,24.58827520879765,24.544836633065476,24.501502522943063,24.45829624877824,24.415211033962393,24.372268348821713,24.32940869875272,24.28661921306181,24.243900057045657,24.20123756868234,24.158635823142664,24.116085811595283,24.07354597041076,24.031024132619027,23.98849459580351,23.945968379116685,23.903447467686195,23.860964445005763,23.818529334395585,23.776153724071182,23.733814181415827,23.691505393068987,23.64922927448007,23.606962352881943,23.56470744761934,23.522436999885766,23.480138442746078,23.437828638878546,23.395506626816392,23.353185780111392,23.310849424206044,23.268492796126743,23.226139073641495,23.18380065581922,23.141475549951647,23.099162233711066,23.05682911478287,23.014475248272458,22.97209342383786,22.929696545173133,22.88727301647469,22.844831200867166,22.802333442150488,22.75979086599513,22.717203747235516,22.674581553695457,22.631954424240963,22.589334979922555,22.546711084911003,22.504074971711052,22.46144245639333,22.418829051182104,22.376206735365635,22.33353388576593,22.290813360376568,22.248061648493263,22.20531894016135,22.162602739999862,22.119888918018837,22.077144636402313,22.034387489606004,21.991590384283366,21.948759090357576,21.905915870456184,21.86307154227916,21.820202213977186,21.777292950150095,21.7343290864768,21.691298544504665,21.6482221358276,21.605106815088707,21.561905284341545,21.51866063110732,21.47539044264792,21.432090639825837,21.388745333755313,21.345353492967114,21.301890481033425,21.258373677003387,21.214813018596693,21.17120188397085,21.12754805690128,21.083847448962942,21.04005632978913,20.996152460575768,20.95214424945301,20.90802276230298,20.86381171242273,20.819501897756012,20.77506412279435,20.73051507145897,20.6858656687359,20.641090492327194,20.5961736461676,20.551110399571215,20.505898878854182,20.46051863976395,20.414992831172846,20.369347382492734,20.3235633074992,20.277606685022164,20.23148176099551,20.185221154730172,20.138830766209843,20.092300697686778,20.045648513704087,19.998875510391322,19.951978145255737,19.90494822246674,19.857795374020437,19.810492243364948,19.763063048229164,19.71551938991827,19.66787050348681,19.62007239383001,19.5721314678144,19.524013232298316,19.475732033080522,19.427254221678226,19.37859731537122,19.32978375494023,19.28075813645885,19.231486943951644,19.181960584323647,19.132221002736287,19.08223788672569,19.032039930369614,18.981629427456557,18.93101690172999,18.880191750999956,18.829144657618695,18.777825935341546,18.72620115188952,18.6742961234401,18.622154172949504,18.569759012767467,18.517152023653875,18.46429579294623,18.411237810032464,18.357979002564587,18.304511484964216,18.250873286977136,18.197075240263086,18.143145224254532,18.089031378963284,18.034767050698118,17.980339179936983,17.925692695065838,17.87083831232636,17.815747593187385,17.760381611434546,17.70477379714002,17.648939208738657,17.59285116370646,17.536529358442337,17.47995636628642,17.42315127160853,17.3661194852167,17.30881988166003,17.25123822330318,17.19338116891369,17.135257744305786,17.07690119824638,17.01831223149047,16.959508565143274,16.900472957935097,16.841224362149372,16.781802708581655,16.722222032718786,16.662533549690412,16.602749721539553,16.542855704698983,16.482855894864564,16.422759567332733,16.362524416015336,16.302151098137646,16.241614148738307,16.180906488207288,16.12002097945054,16.058990811729952,15.997838721013142,15.93657003296626,15.875144264586165,15.813554860567688,15.751821027046482,15.689912234041362,15.627810884330415,15.56551754883513,15.5030178484644,15.440264778706059,15.377301364770094,15.31414707426121,15.250766101709765,15.187101981199591,15.12319172865389,15.059035872879454,14.994634171885302,14.930007397429783,14.865170643311961,14.800088359555504,14.734755678336262,14.669172320091498,14.603349152606556,14.537296254500781,14.471025609026373,14.404560193931083,14.337873378000104,14.270959025718954,14.203819082402493,14.136482601523408,14.068900961721852,14.001028489606465,13.932913134797507,13.864628690195435,13.796171665875056,13.727580565492845,13.658812438880782,13.589832065779127,13.520635101019348,13.451229435914644,13.381595207727766,13.311710262638634,13.241583819483083,13.171208788214324,13.10055824043463,13.02963234677119,12.958442769045682,12.886970167825133,12.815190961344374,12.74311230255144,12.670743269512254,12.598125944360229,12.525246010808427,12.452086409273656,12.378648944704516,12.304912844691676,12.230917075868526,12.156694208184264,12.082217356460646,12.007485046913287,11.932540472525414,11.857399221948443,11.7820230401015,11.706385991539914,11.630488171944275,11.55435831646915,11.478013598316751,11.401438186914357,11.324634735395298,11.247605107323952,11.170383655783983,11.092958997978144,11.015350009517654,10.937561385408188,10.859576211848713,10.781369196653229,10.702939480157983,10.624264783954859,10.545383631838785,10.466335847914452,10.387092678139805,10.307640639655597,10.227958310992461,10.148057261086125,10.067944643967635,9.987577445523574,9.906933336606327,9.826039716144527,9.744896367556843,9.663459415836414,9.581736392232093,9.499727122346215,9.4174038077582,9.334778304009541,9.251871436522736,9.168688517038863,9.085194826500166,9.001360183899036,8.917131914095586,8.832495363520247,8.747462363038812,8.662022224810123,8.57622220653201,8.49007481995614,8.403561985793742,8.316688701687713,8.229475650677939,8.141906260441502,8.054001882670951,7.9658085257383595,7.877343553796454,7.788649784183411,7.699741314595543,7.610581578972464,7.521146972303535,7.431442662432612,7.341487240671941,7.2513192935771364,7.160960659740449,7.070379692412912,6.9795640458526,6.888506016620118,6.797190635510263,6.705599607430465,6.613745177699015,6.521644279096566,6.429286710982646,6.336670883444981,6.243783177524994,6.150605552736951,6.057088712453285,5.963158995267111,5.868807948000109,5.77405198426975,5.678915615921629,5.5834286811158025,5.487620564352376,5.391544390324217,5.29522005166924,5.198677180558416,5.101933082669197,5.005052124969793,4.90805108793137,4.810944152605782,4.713746844898524,4.616474146251011,4.519148472644041,4.421819557440329,4.324516215409349,4.227250323423133,4.130066825951072,4.0330422466071925,3.9362491463026688,3.8397243208357743,3.743500027008723,3.6476289878580235,3.55217657796869,3.457163343137058,3.3626262098929676,3.2686113440112603,3.175159921300623,3.082304346759189,2.9901331281298678,2.8987385583793435,2.8082039221255353,2.718603402313343,2.6300176352124693,2.542502629346798,2.4561401006004546,2.371034922183918,2.287252542811878,2.2048413678098076,2.123839682799602,2.0443380189290465,1.9664001397464785,1.8900605737461114,1.8153410660464078,1.7422696742947201,1.6709068330197265,1.6013009139865395,1.533485163459872,1.4674792220057458,1.4032945777425243,1.3409374601017954,1.2804029995433972,1.2216881626565355,1.1647879486240833,1.109710964717764,1.056460737483406,1.005020227554123,0.9553817493718921,0.9075515438032543,0.8615170230200967,0.8172238612178211,0.7746447729121881,0.73375640160695,0.6945259706223894,0.6569219586560027,0.6209245766704901,0.5864902779894339,0.5535855830936142,0.5221926137143295,0.49227139065029635,0.46378029861878467,0.43668725292794547,0.41095463657196146,0.38654493913077054,0.36341082711610606,0.34149221000797547,0.3207350876832437,0.301100432123691,0.28253938500560954,0.26500815049348714,0.24847016798889895,0.2328750286199112,0.21817954269278117,0.20434861960080009,0.19134557658136475,0.17911787432859616,0.16762676569151957,0.1568319015258889,0.14669324238321851,0.13717693259094546,0.128246800761364,0.11986803184195778,0.11200781831571076,0.1046343414232878,0.09772461925728253,0.09125539645382026,0.08520269495332226,0.07953887312949932,0.07424207535243962,0.06928751777717766,0.06465419769967878,0.06032760892292698],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15,\"font\":{\"size\":20}},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15,\"font\":{\"size\":20}},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05,\"font\":{\"size\":30}},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Rolling Avg Loss\"}},\"legend\":{\"title\":{\"text\":\"Color\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Training Curve for Modular Arithmetic - 35% Addition\"},\"updatemenus\":[{\"active\":-1,\"buttons\":[{\"args\":[{\"xaxis.type\":\"log\"}],\"args2\":[{\"xaxis.type\":\"linear\"}],\"label\":\"Log x-axis\",\"method\":\"relayout\"}],\"type\":\"buttons\",\"x\":-0.1,\"y\":1.0},{\"active\":-1,\"buttons\":[{\"args\":[{\"yaxis.type\":\"log\"}],\"args2\":[{\"yaxis.type\":\"linear\"}],\"label\":\"Log y-axis\",\"method\":\"relayout\"}],\"type\":\"buttons\",\"x\":-0.1,\"y\":0.85}]},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8d84a2b7-cba4-45c6-9af4-04248e513269');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                });            </script>        </div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from neel_plotly.plot import line\n",
        "step = 10\n",
        "average_window = 200\n",
        "train_losses_avg = rolling_average(train_losses, average_window)\n",
        "test_losses_avg = rolling_average(test_losses, average_window)\n",
        "if not ONLY_ADDITION:\n",
        "    add_test_losses_avg = rolling_average(add_test_losses, average_window//10)\n",
        "    multi_test_losses_avg = rolling_average(multi_test_losses, average_window//10)\n",
        "\n",
        "if not ONLY_ADDITION:\n",
        "    line([train_losses[::step], test_losses[::step], add_test_losses, multi_test_losses], x=np.arange(0, len(train_losses_avg), step), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=False, title=f\"Training Curve for Modular Arithmetic - {addition_frac*100:.0f}% Addition\", line_labels=['train loss', 'test loss', 'add loss', 'multi loss'], toggle_x=True, toggle_y=True)\n",
        "    line([train_losses_avg[::step], test_losses_avg[::step], add_test_losses_avg, multi_test_losses_avg], x=np.arange(0, len(train_losses_avg), step), xaxis=\"Epoch\", yaxis=\"Rolling Avg Loss\", log_y=False, title=f\"Training Curve for Modular Arithmetic - {addition_frac*100:.0f}% Addition\", line_labels=['train', 'test', 'add loss', 'multi loss'], toggle_x=True, toggle_y=True)\n",
        "else:\n",
        "    line([train_losses[::step], test_losses[::step]], x=np.arange(0, len(train_losses_avg), step), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=False, title=f\"Training Curve for Modular Arithmetic - {addition_frac*100:.0f}% Addition\", line_labels=['train loss', 'test loss'], toggle_x=True, toggle_y=True)\n",
        "    line([train_losses_avg[::step], test_losses_avg[::step]], x=np.arange(0, len(train_losses_avg), step), xaxis=\"Epoch\", yaxis=\"Rolling Avg Loss\", log_y=False, title=f\"Training Curve for Modular Arithmetic - {addition_frac*100:.0f}% Addition\", line_labels=['train', 'test'], toggle_x=True, toggle_y=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8859a5491331dba93123a91c2831400aced845b502848170e05fcb48b2c144be"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
