{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cil-TiwB2UV1"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Grokking_Demo.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A9Wv1do2UV2"
      },
      "source": [
        "# Grokking Demo Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A13-7Mc62UV3"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Q998xCML2UV3"
      },
      "outputs": [],
      "source": [
        "TRAIN_MODEL = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "zUBTNnz62UV3",
        "outputId": "3658e07b-1cba-42b8-f23b-a319f209561a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running as a Jupyter notebook - intended for development only!\n",
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/85/m4tlmmlj4w58khykxvwgvt980000gn/T/ipykernel_90490/2858945602.py:22: DeprecationWarning:\n",
            "\n",
            "`magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "\n",
            "/var/folders/85/m4tlmmlj4w58khykxvwgvt980000gn/T/ipykernel_90490/2858945602.py:23: DeprecationWarning:\n",
            "\n",
            "`magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "import os\n",
        "\n",
        "DEVELOPMENT_MODE = True\n",
        "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "\n",
        "    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working\n",
        "    # # Install another version of node that makes PySvelte work way faster\n",
        "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
        "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")\n",
        "\n",
        "if IN_COLAB or IN_GITHUB:\n",
        "    %pip install transformer_lens\n",
        "    %pip install circuitsvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "I8nEQDlP2UV4",
        "outputId": "a46ab340-d447-407c-9e3d-36be3ef04ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using renderer: notebook_connected\n"
          ]
        }
      ],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import plotly.io as pio\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "rp4RGtCp2UV4"
      },
      "outputs": [],
      "source": [
        "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.title.font.size = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "RpPUz36r2UV4"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import os\n",
        "import tqdm.auto as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from typing import List, Union, Optional\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "NmIM5yWr2UV4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5bjmWrq2UV5"
      },
      "source": [
        "Plotting helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "aZe03f--2UV5"
      },
      "outputs": [],
      "source": [
        "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EE1YCCg2UV5"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEDLfns2UV5"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "2-6AgR5N2UV5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "DATA_SEED = 598"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqhvJcjT2UV5"
      },
      "source": [
        "## Define Task\n",
        "* Define modular addition\n",
        "* Define the dataset & labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Addition dataset size = 12769\n",
            "Multiplication dataset size = 12769\n",
            "Addition train size = 10215\n",
            "Addition test size = 2554\n",
            "Multiplication train size = 5108\n",
            "Multiplication test size = 7661\n",
            "Addition: 66.7% Multi: 33.3%\n",
            "Combined dataset = 25538\n",
            "Train size = 15323\n",
            "Test size = 10215\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "max_nums = 113\n",
        "mod_value = 113\n",
        "batch_size = 128\n",
        "\n",
        "add_frac = 0.8\n",
        "multi_frac = 0.4\n",
        "\n",
        "def create_dataset(max_nums: int, addition: bool, mod_value: int):\n",
        "    data = list()\n",
        "    for a in range(max_nums):\n",
        "        for b in range(max_nums):\n",
        "            if addition:\n",
        "                label = (a + b) % mod_value\n",
        "            else:\n",
        "                label = (a * b) % mod_value\n",
        "            data.append([[a, b, int(addition)], label])\n",
        "\n",
        "    return pd.DataFrame(data, columns=[\"input\", \"label\"])\n",
        "\n",
        "addition_df = create_dataset(max_nums, True, mod_value)\n",
        "multiplication_df = create_dataset(max_nums, False, mod_value)\n",
        "\n",
        "print(f\"Addition dataset size = {len(addition_df)}\")\n",
        "print(f\"Multiplication dataset size = {len(multiplication_df)}\")\n",
        "\n",
        "# Create the train test splits\n",
        "add_train_df = addition_df.sample(frac=add_frac, random_state=DATA_SEED)\n",
        "add_test_df = addition_df.drop(add_train_df.index)\n",
        "\n",
        "print(f\"Addition train size = {len(add_train_df)}\")\n",
        "print(f\"Addition test size = {len(add_test_df)}\")\n",
        "\n",
        "multi_train_df = multiplication_df.sample(frac=multi_frac, random_state=DATA_SEED)\n",
        "multi_test_df = multiplication_df.drop(multi_train_df.index)\n",
        "\n",
        "print(f\"Multiplication train size = {len(multi_train_df)}\")\n",
        "print(f\"Multiplication test size = {len(multi_test_df)}\")\n",
        "\n",
        "print(f\"Addition: {len(add_train_df)/(len(add_train_df) + len(multi_train_df))*100:0.1f}% Multi: {len(multi_train_df)/(len(add_train_df) + len(multi_train_df))*100:0.1f}%\")\n",
        "\n",
        "train_df = pd.concat([add_train_df, multi_train_df], ignore_index=True)\n",
        "test_df = pd.concat([add_test_df, multi_test_df], ignore_index=True)\n",
        "\n",
        "print(f\"Combined dataset = {len(train_df) + len(test_df)}\")\n",
        "\n",
        "print(f\"Train size = {len(train_df)}\")\n",
        "print(f\"Test size = {len(test_df)}\")\n",
        "\n",
        "# Create the dataloaders\n",
        "def get_dataloader(df, batch_size, shuffle=True):\n",
        "    inputs = torch.tensor(df['input'].tolist())\n",
        "    labels = torch.tensor(df['label'].tolist())\n",
        "    dataset = TensorDataset(inputs, labels)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "add_test_loader = get_dataloader(add_test_df, batch_size, shuffle=False)\n",
        "multi_test_loader = get_dataloader(multi_test_df, batch_size, shuffle=False)\n",
        "\n",
        "train_loader = get_dataloader(train_df, batch_size, shuffle=True)\n",
        "test_loader = get_dataloader(test_df, batch_size, shuffle=False)\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv0e7P-W2UV6"
      },
      "source": [
        "Convert this to a train + test set - 30% in the training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcd2dy9x2UV6"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "GO7-UYjQ2UV6"
      },
      "outputs": [],
      "source": [
        "\n",
        "cfg = HookedTransformerConfig(\n",
        "    n_layers = 1,\n",
        "    n_heads = 4,\n",
        "    d_model = 128,\n",
        "    d_head = 32,\n",
        "    d_mlp = 512,\n",
        "    act_fn = \"relu\",\n",
        "    normalization_type=\"LN\",\n",
        "    d_vocab=max_nums+1,\n",
        "    d_vocab_out=mod_value,\n",
        "    n_ctx=dataset.size(1),\n",
        "    init_weights=True,\n",
        "    device=device,\n",
        "    seed = 999,\n",
        ")\n",
        "\n",
        "model = HookedTransformer(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qNAf_pY2UV6"
      },
      "source": [
        "Disable the biases, as we don't need them for this task and it makes things easier to interpret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "9Y6S95XF2UV6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "for name, param in model.named_parameters():\n",
        "    if \"b_\" in name:\n",
        "        param.requires_grad = False\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQtU2xX02UV6"
      },
      "source": [
        "## Define Optimizer + Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "kt9M_v7a2UV6"
      },
      "outputs": [],
      "source": [
        "# Optimizer config\n",
        "lr = 1e-3\n",
        "wd = 1.\n",
        "betas = (0.9, 0.98)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Aa0E-GvJ2UV6",
        "outputId": "9a149f5d-53d0-42ce-c387-f07f6360c5cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(5.0193, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
            "tensor(5.0189, dtype=torch.float64, grad_fn=<NegBackward0>)\n"
          ]
        }
      ],
      "source": [
        "def loss_fn(logits, labels):\n",
        "    if len(logits.shape)==3:\n",
        "        logits = logits[:, -1]\n",
        "    logits = logits.to(torch.float64)\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]\n",
        "    return -correct_log_probs.mean()\n",
        "train_logits = model(train_data)\n",
        "train_loss = loss_fn(train_logits, train_labels)\n",
        "print(train_loss)\n",
        "test_logits = model(test_data)\n",
        "test_loss = loss_fn(test_logits, test_labels)\n",
        "print(test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOuSwO1U2UV7"
      },
      "source": [
        "## Actually Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NexUPO8W2UV7"
      },
      "source": [
        "**Weird Decision:** Training the model with full batch training rather than stochastic gradient descent. We do this so to make training smoother and reduce the number of slingshots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c477d3a6463646deb7cf1de99bb0199a"
          ]
        },
        "id": "B78ji8Si2UV7",
        "outputId": "db8171c9-35a3-497a-aa61-a9b8efeee374"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "839e0945217f4f008fd52abf17931564",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/6000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[50], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m train_logits \u001b[38;5;241m=\u001b[39m model(train_data)\n\u001b[1;32m     10\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m loss_fn(train_logits, train_labels)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(f\"train_loss.item() = {train_loss.item()}\")\u001b[39;00m\n",
            "File \u001b[0;32m~/dev/ArithmeticTransformer/.venv/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/dev/ArithmeticTransformer/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/dev/ArithmeticTransformer/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 6000\n",
        "checkpoint_every = 100\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "model_checkpoints = []\n",
        "checkpoint_epochs = []\n",
        "if TRAIN_MODEL:\n",
        "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
        "        train_logits = model(train_data)\n",
        "        train_loss = loss_fn(train_logits, train_labels)\n",
        "        train_loss.backward()\n",
        "        train_losses.append(train_loss.item())\n",
        "        # print(f\"train_loss.item() = {train_loss.item()}\")\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            test_logits = model(test_data)\n",
        "            test_loss = loss_fn(test_logits, test_labels)\n",
        "            test_losses.append(test_loss.item())\n",
        "\n",
        "        if ((epoch+1)%checkpoint_every)==0:\n",
        "            checkpoint_epochs.append(epoch)\n",
        "            model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
        "            print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}\")\n",
        "            # print(f\"len(train_losses) = {len(train_losses)} len(test_losses) = {len(test_losses)} len(model_checkpoints) = {len(model_checkpoints)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "UrvOo2Qs2UV7"
      },
      "outputs": [],
      "source": [
        "PTH_LOCATION = \"../saves/grokking_add_and_multi.pth\"\n",
        "if TRAIN_MODEL:\n",
        "    # Create the directory if it does not exist\n",
        "    os.makedirs(Path(PTH_LOCATION).parent, exist_ok=True)\n",
        "    \n",
        "    print(f\"len(train_losses) = {len(train_losses)} len(test_losses) = {len(test_losses)} len(model_checkpoints) = {len(model_checkpoints)}\")\n",
        "    torch.save(\n",
        "        {\n",
        "            \"model\":model.state_dict(),\n",
        "            \"config\": model.cfg,\n",
        "            \"checkpoints\": model_checkpoints,\n",
        "            \"checkpoint_epochs\": checkpoint_epochs,\n",
        "            \"test_losses\": test_losses,\n",
        "            \"train_losses\": train_losses,\n",
        "            \"train_indices\": train_indices,\n",
        "            \"test_indices\": test_indices,\n",
        "        },\n",
        "        PTH_LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "tx5DVueg2UV7"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for HookedTransformer:\n\tMissing key(s) in state_dict: \"blocks.0.ln1.w\", \"blocks.0.ln1.b\", \"blocks.0.ln2.w\", \"blocks.0.ln2.b\", \"ln_final.w\", \"ln_final.b\". ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[34], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TRAIN_MODEL:\n\u001b[1;32m      3\u001b[0m     cached_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(PTH_LOCATION, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     model_checkpoints \u001b[38;5;241m=\u001b[39m cached_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m     checkpoint_epochs \u001b[38;5;241m=\u001b[39m cached_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m~/dev/ArithmeticTransformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for HookedTransformer:\n\tMissing key(s) in state_dict: \"blocks.0.ln1.w\", \"blocks.0.ln1.b\", \"blocks.0.ln2.w\", \"blocks.0.ln2.b\", \"ln_final.w\", \"ln_final.b\". "
          ]
        }
      ],
      "source": [
        "LOAD_LOCATION = \"../saves/grokking_add_and_multi.pth\"\n",
        "if not TRAIN_MODEL:\n",
        "    cached_data = torch.load(PTH_LOCATION, weights_only=False)\n",
        "    model.load_state_dict(cached_data['model'])\n",
        "    model_checkpoints = cached_data[\"checkpoints\"]\n",
        "    checkpoint_epochs = cached_data[\"checkpoint_epochs\"]\n",
        "    test_losses = cached_data['test_losses']\n",
        "    train_losses = cached_data['train_losses']\n",
        "    train_indices = cached_data[\"train_indices\"]\n",
        "    test_indices = cached_data[\"test_indices\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHA9-QD52UV7"
      },
      "source": [
        "## Show Model Training Statistics, Check that it groks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 700,
      "metadata": {
        "id": "mHu_Yz8i2UV7",
        "outputId": "ff53e647-20f1-4a7d-8dae-f680164305f0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>                            <div id=\"93de67ce-24fe-462f-9005-9eae65ec0130\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"93de67ce-24fe-462f-9005-9eae65ec0130\")) {                    Plotly.newPlot(                        \"93de67ce-24fe-462f-9005-9eae65ec0130\",                        [{\"hovertemplate\":\"Color=train\\u003cbr\\u003eEpoch=%{x}\\u003cbr\\u003eLoss=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"train\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,5000,5100,5200,5300,5400,5500,5600,5700,5800,5900],\"xaxis\":\"x\",\"y\":[5.01930433836579,3.0271226541693363,0.9055819125259826,0.19853461373881517,0.06678795486864826,0.04473772912269931,0.4545569951327673,0.03633177402733898,0.02512450282861609,0.04546308419930642,0.7108091651214236,0.04910250525810373,0.11798386745814586,0.032591380660113904,0.04925463146358783,0.02957244957702842,0.039218789044861264,0.0222368539689148,0.010877950231694855,0.021823640065786434,0.014958996397860627,0.007041437281119923,0.002923336147626425,0.0012777432863688974,0.06020305458342413,0.008580385027840129,0.006708205635823672,0.003553495161522822,0.0016031695751443933,0.0007092547081966016,0.00031435952245278574,0.012182460569108237,0.004555854666810304,0.003931588501761544,0.002224403382675068,0.0010194056829500445,0.00045484290809399725,0.0002033413514765867,9.188148132640854e-05,0.018597305784864113,0.0036920281443131,0.003656715216037132,0.0025257459267922526,0.0011942608503778698,0.0005529778132527914,0.0002495912915696548,0.00011440088738828943,5.1406998005834524e-05,0.005190400212194779,0.004449426121867588,0.004103131196507675,0.015416269496818139,0.017913454437115946,0.011705595593000885,0.005526228332824642,0.002512703257984203,0.001125899013163374,0.0004997660038444767,0.00022524779276490484,0.00010451266725998243],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Color=test\\u003cbr\\u003eEpoch=%{x}\\u003cbr\\u003eLoss=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"1\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"test\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,5000,5100,5200,5300,5400,5500,5600,5700,5800,5900],\"xaxis\":\"x\",\"y\":[4.912035624362126,5.62743639753654,6.694631106244484,7.5288637872433215,7.929526880245574,8.005924302959913,7.936291503732276,7.767386336740455,7.778097450844641,7.024364084726791,6.713986388827383,6.010846420749674,5.250153952600304,4.831565160448442,3.575682752082475,3.11146202197592,1.7290638025280767,1.2845123492565604,0.7133929875910827,0.3524760177756227,0.2108800120536794,0.07770348075593499,0.022937153140227085,0.007540118141446069,0.15450349905670402,0.026870691272527416,0.017814490120885648,0.009065023819723841,0.004106949752520378,0.0018529615838977428,0.0008563009035610306,0.03236426957387622,0.010633048717595793,0.00790718832404552,0.004418043950559576,0.0020892525592198403,0.000974074493175773,0.00045846869518918683,0.00021998222983982113,0.043896584491612746,0.008270674056501774,0.0066466713459700415,0.0042789848922688755,0.0021481820770316057,0.001034267640681739,0.00048510041349791617,0.00023455800608723816,0.00011365065864847365,0.014106100830850439,0.009028465606627092,0.007047545996097254,0.02781294218897122,0.027930220923379965,0.018084445820783186,0.008974055495287809,0.004358552611655173,0.0020853505883320954,0.0010037355217130215,0.0004855024405993238,0.00023865422025153197],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15,\"font\":{\"size\":20}},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15,\"font\":{\"size\":20}},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05,\"font\":{\"size\":30}},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss\"}},\"legend\":{\"title\":{\"text\":\"Color\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Training Curve for Modular Arithmetic\"},\"updatemenus\":[{\"active\":-1,\"buttons\":[{\"args\":[{\"xaxis.type\":\"log\"}],\"args2\":[{\"xaxis.type\":\"linear\"}],\"label\":\"Log x-axis\",\"method\":\"relayout\"}],\"type\":\"buttons\",\"x\":-0.1,\"y\":1.0},{\"active\":-1,\"buttons\":[{\"args\":[{\"yaxis.type\":\"log\"}],\"args2\":[{\"yaxis.type\":\"linear\"}],\"label\":\"Log y-axis\",\"method\":\"relayout\"}],\"type\":\"buttons\",\"x\":-0.1,\"y\":0.85}]},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('93de67ce-24fe-462f-9005-9eae65ec0130');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                });            </script>        </div>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from neel_plotly.plot import line\n",
        "line([train_losses[::100], test_losses[::100]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=False, title=\"Training Curve for Modular Arithmetic\", line_labels=['train', 'test'], toggle_x=True, toggle_y=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN897frZ2UV7"
      },
      "source": [
        "# Analysing the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lky2UlrN2UV7"
      },
      "source": [
        "## Standard Things to Try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ad1cjmw2UV7",
        "outputId": "40290216-d392-452e-ef1b-0575d60908c4"
      },
      "outputs": [],
      "source": [
        "original_logits, cache = model.run_with_cache(dataset)\n",
        "print(original_logits.numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_32StlUr2UV8"
      },
      "source": [
        "Get key weight matrices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0lbOxvs2UV8",
        "outputId": "c500b9a8-1697-4abe-a247-883532a1e436"
      },
      "outputs": [],
      "source": [
        "W_E = model.embed.W_E[:-1]\n",
        "print(\"W_E\", W_E.shape)\n",
        "W_neur = W_E @ model.blocks[0].attn.W_V @ model.blocks[0].attn.W_O @ model.blocks[0].mlp.W_in\n",
        "print(\"W_neur\", W_neur.shape)\n",
        "W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
        "print(\"W_logit\", W_logit.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWSkeC1e2UV8",
        "outputId": "f18b80b4-a037-4c45-fb87-0aaa8b6ed942"
      },
      "outputs": [],
      "source": [
        "original_loss = loss_fn(original_logits, labels).item()\n",
        "print(\"Original Loss:\", original_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQs-TLVD2UV8"
      },
      "source": [
        "### Looking at Activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_u-8AXN2UV8"
      },
      "source": [
        "Helper variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMRzkNxW2UV8"
      },
      "outputs": [],
      "source": [
        "pattern_a = cache[\"pattern\", 0, \"attn\"][:, :, -1, 0]\n",
        "pattern_b = cache[\"pattern\", 0, \"attn\"][:, :, -1, 1]\n",
        "neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]\n",
        "neuron_pre_acts = cache[\"pre\", 0, \"mlp\"][:, -1, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi43QmAV2UV8"
      },
      "source": [
        "Get all shapes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44O81Wt12UV8",
        "outputId": "7b92074e-d417-410d-e832-044cfb2c0a9e"
      },
      "outputs": [],
      "source": [
        "for param_name, param in cache.items():\n",
        "    print(param_name, param.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2tGOA822UV8",
        "outputId": "4930b366-5eef-43a6-e596-b3f3ede92329"
      },
      "outputs": [],
      "source": [
        "imshow(cache[\"pattern\", 0].mean(dim=0)[:, -1, :], title=\"Average Attention Pattern per Head\", xaxis=\"Source\", yaxis=\"Head\", x=['a', 'b', '='])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXFSWBrl2UV8",
        "outputId": "51f7df84-72c4-48f1-dd60-a5b0436b182a"
      },
      "outputs": [],
      "source": [
        "imshow(cache[\"pattern\", 0][5][:, -1, :], title=\"Average Attention Pattern per Head\", xaxis=\"Source\", yaxis=\"Head\", x=['a', 'b', '='])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvHDHQcP2UV9",
        "outputId": "26485c31-fd76-4cfd-c2b6-fef71360d054"
      },
      "outputs": [],
      "source": [
        "dataset[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhjwoXGe2UV9",
        "outputId": "ddb29030-fe5c-46db-9c4f-b817a951572b"
      },
      "outputs": [],
      "source": [
        "imshow(cache[\"pattern\", 0][:, 0, -1, 0].reshape(p, p), title=\"Attention for Head 0 from a -> =\", xaxis=\"b\", yaxis=\"a\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LwbcjFj2UV9",
        "outputId": "a46be588-14b4-4d28-945c-0acbdf8ac13d"
      },
      "outputs": [],
      "source": [
        "imshow(\n",
        "    einops.rearrange(cache[\"pattern\", 0][:, :, -1, 0], \"(a b) head -> head a b\", a=mod_value, b=mod_value),\n",
        "    title=\"Attention for Head 0 from a -> =\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C18Gctun2UV9"
      },
      "source": [
        "Plotting neuron activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFQzVdMM2UV9",
        "outputId": "4ac4c82c-5914-4efb-8845-edf48b016c78"
      },
      "outputs": [],
      "source": [
        "cache[\"post\", 0, \"mlp\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tz_2XGR2UV9",
        "outputId": "a77b881a-262d-4245-d92c-9a746d1e7577"
      },
      "outputs": [],
      "source": [
        "imshow(\n",
        "    einops.rearrange(neuron_acts[:, :5], \"(a b) neuron -> neuron a b\", a=mod_value, b=mod_value),\n",
        "    title=\"First 5 neuron acts\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoOcLnhm2UV9"
      },
      "source": [
        "### Singular Value Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE-XXggQ2UV9",
        "outputId": "12cfad9b-b58f-410f-c13a-d91fd5dcb189"
      },
      "outputs": [],
      "source": [
        "W_E.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07Bm-WMn2UV9",
        "outputId": "739d044a-c6dd-4903-f147-0200378d47a1"
      },
      "outputs": [],
      "source": [
        "U, S, Vh = torch.svd(W_E)\n",
        "line(S, title=\"Singular Values\")\n",
        "imshow(U, title=\"Principal Components on the Input\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OGIPYYb2UV9",
        "outputId": "2fc80913-97eb-4d6e-dbef-5f24cb80a968"
      },
      "outputs": [],
      "source": [
        "# Control - random Gaussian matrix\n",
        "U, S, Vh = torch.svd(torch.randn_like(W_E))\n",
        "line(S, title=\"Singular Values Random\")\n",
        "imshow(U, title=\"Principal Components Random\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4F8XwEN2UV-"
      },
      "source": [
        "## Explaining Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2p89_lv2UV-"
      },
      "source": [
        "### Analyse the Embedding - It's a Lookup Table!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpwI-2C82UV-",
        "outputId": "243d9d45-b006-4065-8eda-34ab8e36c3d8"
      },
      "outputs": [],
      "source": [
        "U, S, Vh = torch.svd(W_E)\n",
        "line(U[:, :8].T, title=\"Principal Components of the embedding\", xaxis=\"Input Vocabulary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNHR2cO02UV-",
        "outputId": "d90d0575-e499-4601-f930-d1e29e523e90"
      },
      "outputs": [],
      "source": [
        "fourier_basis = []\n",
        "fourier_basis_names = []\n",
        "fourier_basis.append(torch.ones(p))\n",
        "fourier_basis_names.append(\"Constant\")\n",
        "for freq in range(1, p//2+1):\n",
        "    fourier_basis.append(torch.sin(torch.arange(p)*2 * torch.pi * freq / p))\n",
        "    fourier_basis_names.append(f\"Sin {freq}\")\n",
        "    fourier_basis.append(torch.cos(torch.arange(p)*2 * torch.pi * freq / p))\n",
        "    fourier_basis_names.append(f\"Cos {freq}\")\n",
        "fourier_basis = torch.stack(fourier_basis, dim=0).to(device)\n",
        "fourier_basis = fourier_basis/fourier_basis.norm(dim=-1, keepdim=True)\n",
        "imshow(fourier_basis, xaxis=\"Input\", yaxis=\"Component\", y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpTQ6pZ62UV-",
        "outputId": "8bd7647d-b083-4796-e2d8-8d91bb110a26"
      },
      "outputs": [],
      "source": [
        "line(fourier_basis[:8], xaxis=\"Input\", line_labels=fourier_basis_names[:8], title=\"First 8 Fourier Components\")\n",
        "line(fourier_basis[25:29], xaxis=\"Input\", line_labels=fourier_basis_names[25:29], title=\"Middle Fourier Components\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyYTvK0T2UV-",
        "outputId": "a50cdb18-cad8-461e-f39c-6d5a00af61e6"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis @ fourier_basis.T, title=\"All Fourier Vectors are Orthogonal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5axBeip2UV-"
      },
      "source": [
        "### Analyse the Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIYWBRxL2UV-",
        "outputId": "f1797c1f-f193-46d8-9db6-478e3fcac0ff"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis @ W_E, yaxis=\"Fourier Component\", xaxis=\"Residual Stream\", y=fourier_basis_names, title=\"Embedding in Fourier Basis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DGzeqL52UV-",
        "outputId": "23e09b71-65fd-4bea-a601-e8fecc20c580"
      },
      "outputs": [],
      "source": [
        "line((fourier_basis @ W_E).norm(dim=-1), xaxis=\"Fourier Component\", x=fourier_basis_names, title=\"Norms of Embedding in Fourier Basis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TChjDes2UV-",
        "outputId": "d37ec48f-8ee0-4dcc-c1ab-c5cf45822ce0"
      },
      "outputs": [],
      "source": [
        "key_freqs = [17, 25, 32, 47]\n",
        "key_freq_indices = [33, 34, 49, 50, 63, 64, 93, 94]\n",
        "fourier_embed = fourier_basis @ W_E\n",
        "key_fourier_embed = fourier_embed[key_freq_indices]\n",
        "print(\"key_fourier_embed\", key_fourier_embed.shape)\n",
        "imshow(key_fourier_embed @ key_fourier_embed.T, title=\"Dot Product of embedding of key Fourier Terms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w3IyUz82UV-"
      },
      "source": [
        "### Key Frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIgPAisZ2UV-",
        "outputId": "66e71cfc-b345-4edc-9106-1f47c4ac51e6"
      },
      "outputs": [],
      "source": [
        "line(fourier_basis[[34, 50, 64, 94]], title=\"Cos of key freqs\", line_labels=[34, 50, 64, 94])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqxItZbN2UV_",
        "outputId": "e7a95003-7c80-4e79-958a-13eead2e008c"
      },
      "outputs": [],
      "source": [
        "line(fourier_basis[[34, 50, 64, 94]].mean(0), title=\"Constructive Interference\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJOGfHBJ2UV_"
      },
      "source": [
        "## Analyse Neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN2BdryQ2UV_",
        "outputId": "82a0cff2-1ae7-4887-cf3c-232a7c46458a"
      },
      "outputs": [],
      "source": [
        "imshow(\n",
        "    einops.rearrange(neuron_acts[:, :5], \"(a b) neuron -> neuron a b\", a=p, b=p),\n",
        "    title=\"First 5 neuron acts\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwSwA7BB2UV_",
        "outputId": "280626ae-4052-4d28-89a9-814ce431eec7"
      },
      "outputs": [],
      "source": [
        "imshow(\n",
        "    einops.rearrange(neuron_acts[:, 0], \"(a b) -> a b\", a=p, b=p),\n",
        "    title=\"First neuron act\", xaxis=\"b\", yaxis=\"a\",)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKgpNncB2UV_",
        "outputId": "06cb8e00-12c4-47bd-d30f-c42b926400fc"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis[94][None, :] * fourier_basis[94][:, None], title=\"Cos 47a * cos 47b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_CfsHhN2UV_",
        "outputId": "217e41fd-7b71-408d-a25b-76b96f71a23c"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis[94][None, :] * fourier_basis[0][:, None], title=\"Cos 47a * const\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPO8kd6z2UV_",
        "outputId": "6b379743-c9fa-4753-e16b-fd380ff2cc25"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis @ neuron_acts[:, 0].reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of neuron 0\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h2_i8WS2UV_",
        "outputId": "caad8394-6533-4e40-9c57-f19d06d187f7"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis @ neuron_acts[:, 5].reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of neuron 5\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU6fmQau2UV_",
        "outputId": "fec9253d-be02-48b2-b1e1-f9742d130bbf"
      },
      "outputs": [],
      "source": [
        "imshow(fourier_basis @ torch.randn_like(neuron_acts[:, 0]).reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of RANDOM\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VALEk5FF2UV_"
      },
      "source": [
        "### Neuron Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INkgFDrp2UV_",
        "outputId": "2f5d5b18-8e71-4604-90a3-b6286f455a45"
      },
      "outputs": [],
      "source": [
        "fourier_neuron_acts = fourier_basis @ einops.rearrange(neuron_acts, \"(a b) neuron -> neuron a b\", a=p, b=p) @ fourier_basis.T\n",
        "# Center these by removing the mean - doesn't matter!\n",
        "fourier_neuron_acts[:, 0, 0] = 0.\n",
        "print(\"fourier_neuron_acts\", fourier_neuron_acts.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv5mnIue2UV_",
        "outputId": "25f8e51c-c61b-4ce3-842c-0d2eb3312c26"
      },
      "outputs": [],
      "source": [
        "neuron_freq_norm = torch.zeros(p//2, model.cfg.d_mlp).to(device)\n",
        "for freq in range(0, p//2):\n",
        "    for x in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
        "        for y in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
        "            neuron_freq_norm[freq] += fourier_neuron_acts[:, x, y]**2\n",
        "neuron_freq_norm = neuron_freq_norm / fourier_neuron_acts.pow(2).sum(dim=[-1, -2])[None, :]\n",
        "imshow(neuron_freq_norm, xaxis=\"Neuron\", yaxis=\"Freq\", y=torch.arange(1, p//2+1), title=\"Neuron Frac Explained by Freq\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRUBnAkR2UV_",
        "outputId": "0bba5e09-677e-4422-8470-3c5e7b8b26b5"
      },
      "outputs": [],
      "source": [
        "line(neuron_freq_norm.max(dim=0).values.sort().values, xaxis=\"Neuron\", title=\"Max Neuron Frac Explained over Freqs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROyJpjBu2UWA"
      },
      "source": [
        "## Read Off the Neuron-Logit Weights to Interpret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwrTuOuU2UWA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4_iAERQ2UWA",
        "outputId": "ea30ddf6-0e2f-4a77-e8c4-a039f05149a6"
      },
      "outputs": [],
      "source": [
        "W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
        "print(\"W_logit\", W_logit.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-pIY_GA2UWA",
        "outputId": "8cfd60a2-327a-4fd3-8a2e-fa8da103b855"
      },
      "outputs": [],
      "source": [
        "line((W_logit @ fourier_basis.T).norm(dim=0), x=fourier_basis_names, title=\"W_logit in the Fourier Basis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5S6BRPXn2UWA",
        "outputId": "c792848c-f21d-4ad2-f166-dd0616516c51"
      },
      "outputs": [],
      "source": [
        "neurons_17 = neuron_freq_norm[17-1]>0.85\n",
        "neurons_17.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAoHR8om2UWA",
        "outputId": "75976bb5-9bd5-4683-a424-2307486655e6"
      },
      "outputs": [],
      "source": [
        "neurons_17.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItQl2uZH2UWA",
        "outputId": "d753b3c1-d98f-4998-be39-4c9b15302516"
      },
      "outputs": [],
      "source": [
        "line((W_logit[neurons_17] @ fourier_basis.T).norm(dim=0), x=fourier_basis_names, title=\"W_logit for freq 17 neurons in the Fourier Basis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUzoRXjv2UWA"
      },
      "source": [
        "Study sin 17"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QULuLWO2UWA",
        "outputId": "cc133251-ba40-4a60-a832-a6cd996475a7"
      },
      "outputs": [],
      "source": [
        "freq = 17\n",
        "W_logit_fourier = W_logit @ fourier_basis\n",
        "neurons_sin_17 = W_logit_fourier[:, 2*freq-1]\n",
        "line(neurons_sin_17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4PiyHIZ2UWA",
        "outputId": "0822fc3c-0665-4d7a-c597-45232ceb0b6c"
      },
      "outputs": [],
      "source": [
        "neuron_acts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3rL4HX02UWA",
        "outputId": "e0ca3817-ddff-4278-e812-9b1f7fcfb24c"
      },
      "outputs": [],
      "source": [
        "inputs_sin_17c = neuron_acts @ neurons_sin_17\n",
        "imshow(fourier_basis @ inputs_sin_17c.reshape(p, p) @ fourier_basis.T, title=\"Fourier Heatmap over inputs for sin17c\", x=fourier_basis_names, y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiIpOKim2UWA"
      },
      "source": [
        "# Black Box Methods + Progress Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hcb1hzmn2UWB"
      },
      "source": [
        "## Setup Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7RR0FPS2UWB"
      },
      "source": [
        "Code to plot embedding freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg7iSMKC2UWB"
      },
      "outputs": [],
      "source": [
        "def embed_to_cos_sin(fourier_embed):\n",
        "    if len(fourier_embed.shape) == 1:\n",
        "        return torch.stack([fourier_embed[1::2], fourier_embed[2::2]])\n",
        "    else:\n",
        "        return torch.stack([fourier_embed[:, 1::2], fourier_embed[:, 2::2]], dim=1)\n",
        "\n",
        "from neel_plotly.plot import melt\n",
        "\n",
        "def plot_embed_bars(\n",
        "    fourier_embed,\n",
        "    title=\"Norm of embedding of each Fourier Component\",\n",
        "    return_fig=False,\n",
        "    **kwargs\n",
        "):\n",
        "    cos_sin_embed = embed_to_cos_sin(fourier_embed)\n",
        "    df = melt(cos_sin_embed)\n",
        "    # display(df)\n",
        "    group_labels = {0: \"sin\", 1: \"cos\"}\n",
        "    df[\"Trig\"] = df[\"0\"].map(lambda x: group_labels[x])\n",
        "    fig = px.bar(\n",
        "        df,\n",
        "        barmode=\"group\",\n",
        "        color=\"Trig\",\n",
        "        x=\"1\",\n",
        "        y=\"value\",\n",
        "        labels={\"1\": \"$w_k$\", \"value\": \"Norm\"},\n",
        "        title=title,\n",
        "        **kwargs\n",
        "    )\n",
        "    fig.update_layout(dict(legend_title=\"\"))\n",
        "\n",
        "    if return_fig:\n",
        "        return fig\n",
        "    else:\n",
        "        fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otzQkRuy2UWB"
      },
      "source": [
        "Code to test a tensor of edited logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg3m_8CQ2UWB"
      },
      "outputs": [],
      "source": [
        "def test_logits(logits, bias_correction=False, original_logits=None, mode=\"all\"):\n",
        "    # Calculates cross entropy loss of logits representing a batch of all p^2\n",
        "    # possible inputs\n",
        "    # Batch dimension is assumed to be first\n",
        "    if logits.shape[1] == p * p:\n",
        "        logits = logits.T\n",
        "    if logits.shape == torch.Size([p * p, p + 1]):\n",
        "        logits = logits[:, :-1]\n",
        "    logits = logits.reshape(p * p, p)\n",
        "    if bias_correction:\n",
        "        # Applies bias correction - we correct for any missing bias terms,\n",
        "        # independent of the input, by centering the new logits along the batch\n",
        "        # dimension, and then adding the average original logits across all inputs\n",
        "        logits = (\n",
        "            einops.reduce(original_logits - logits, \"batch ... -> ...\", \"mean\") + logits\n",
        "        )\n",
        "    if mode == \"train\":\n",
        "        return loss_fn(logits[train_indices], labels[train_indices])\n",
        "    elif mode == \"test\":\n",
        "        return loss_fn(logits[test_indices], labels[test_indices])\n",
        "    elif mode == \"all\":\n",
        "        return loss_fn(logits, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkzZ7bok2UWB"
      },
      "source": [
        "Code to run a metric over every checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WavCgES2UWB"
      },
      "outputs": [],
      "source": [
        "metric_cache = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUsBnDvB2UWB"
      },
      "outputs": [],
      "source": [
        "def get_metrics(model, metric_cache, metric_fn, name, reset=False):\n",
        "    if reset or (name not in metric_cache) or (len(metric_cache[name]) == 0):\n",
        "        metric_cache[name] = []\n",
        "        for c, sd in enumerate(tqdm.tqdm((model_checkpoints))):\n",
        "            model.reset_hooks()\n",
        "            model.load_state_dict(sd)\n",
        "            out = metric_fn(model)\n",
        "            if type(out) == torch.Tensor:\n",
        "                out = utils.to_numpy(out)\n",
        "            metric_cache[name].append(out)\n",
        "        model.load_state_dict(model_checkpoints[-1])\n",
        "        try:\n",
        "            metric_cache[name] = torch.tensor(metric_cache[name])\n",
        "        except:\n",
        "            metric_cache[name] = torch.tensor(np.array(metric_cache[name]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK2FTxEI2UWC"
      },
      "source": [
        "## Defining Progress Measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2KsOuyM2UWC"
      },
      "source": [
        "### Loss Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXhKHkXD2UWC"
      },
      "outputs": [],
      "source": [
        "memorization_end_epoch = 1500\n",
        "circuit_formation_end_epoch = 13300\n",
        "cleanup_end_epoch = 16600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK6RI5it2UWC"
      },
      "outputs": [],
      "source": [
        "def add_lines(figure):\n",
        "    figure.add_vline(memorization_end_epoch, line_dash=\"dash\", opacity=0.7)\n",
        "    figure.add_vline(circuit_formation_end_epoch, line_dash=\"dash\", opacity=0.7)\n",
        "    figure.add_vline(cleanup_end_epoch, line_dash=\"dash\", opacity=0.7)\n",
        "    return figure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcYtNoui2UWC",
        "outputId": "84186b0e-bf8d-4dd9-92cc-8522cb53b0ed"
      },
      "outputs": [],
      "source": [
        "fig = line([train_losses[::100], test_losses[::100]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Training Curve for Modular Addition\", line_labels=['train', 'test'], toggle_x=True, toggle_y=True, return_fig=True)\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvR9qcZ62UWC"
      },
      "source": [
        "### Logit Periodicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_0fV89s2UWC",
        "outputId": "3ee892ae-7363-4ff0-ba81-48478fc80f9f"
      },
      "outputs": [],
      "source": [
        "all_logits = original_logits[:, -1, :]\n",
        "print(all_logits.shape)\n",
        "all_logits = einops.rearrange(all_logits, \"(a b) c -> a b c\", a=p, b=p)\n",
        "print(all_logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ_Ex_ph2UWC",
        "outputId": "e6ead41d-2f9b-46be-eb03-cbc61e02bf19"
      },
      "outputs": [],
      "source": [
        "coses = {}\n",
        "for freq in key_freqs:\n",
        "    print(\"Freq:\", freq)\n",
        "    a = torch.arange(p)[:, None, None]\n",
        "    b = torch.arange(p)[None, :, None]\n",
        "    c = torch.arange(p)[None, None, :]\n",
        "    cube_predicted_logits = torch.cos(freq * 2 * torch.pi / p * (a + b - c)).to(device)\n",
        "    cube_predicted_logits /= cube_predicted_logits.norm()\n",
        "    coses[freq] = cube_predicted_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRgWuabh2UWC",
        "outputId": "a449306e-cfa8-4b66-d68a-9db843cb778d"
      },
      "outputs": [],
      "source": [
        "approximated_logits = torch.zeros_like(all_logits)\n",
        "for freq in key_freqs:\n",
        "    print(\"Freq:\", freq)\n",
        "    coeff = (all_logits * coses[freq]).sum()\n",
        "    print(\"Coeff:\", coeff)\n",
        "    cosine_sim = coeff / all_logits.norm()\n",
        "    print(\"Cosine Sim:\", cosine_sim)\n",
        "    approximated_logits += coeff * coses[freq]\n",
        "residual = all_logits - approximated_logits\n",
        "print(\"Residual size:\", residual.norm())\n",
        "print(\"Residual fraction of norm:\", residual.norm()/all_logits.norm())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnEO6Kj32UWC",
        "outputId": "c342b319-3128-44ac-9fcc-274f6c4f190f"
      },
      "outputs": [],
      "source": [
        "random_logit_cube = torch.randn_like(all_logits)\n",
        "print((all_logits * random_logit_cube).sum()/random_logit_cube.norm()/all_logits.norm())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3biEyWhq2UWC",
        "outputId": "afeeddec-b719-4dfa-a82f-3290e1a8d4f3"
      },
      "outputs": [],
      "source": [
        "test_logits(all_logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rOuQB4-2UWC",
        "outputId": "e2b7a101-d930-4206-8df9-5918126c9b3b"
      },
      "outputs": [],
      "source": [
        "test_logits(approximated_logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhrjC1y32UWC"
      },
      "source": [
        "#### Look During Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqIMTFib2UWC",
        "outputId": "2b4645eb-6214-451f-f27f-62fb594c1382"
      },
      "outputs": [],
      "source": [
        "cos_cube = []\n",
        "for freq in range(1, p//2 + 1):\n",
        "    a = torch.arange(p)[:, None, None]\n",
        "    b = torch.arange(p)[None, :, None]\n",
        "    c = torch.arange(p)[None, None, :]\n",
        "    cube_predicted_logits = torch.cos(freq * 2 * torch.pi / p * (a + b - c)).to(device)\n",
        "    cube_predicted_logits /= cube_predicted_logits.norm()\n",
        "    cos_cube.append(cube_predicted_logits)\n",
        "cos_cube = torch.stack(cos_cube, dim=0)\n",
        "print(cos_cube.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "4d3444ac0c09485bb2297c35246614f4"
          ]
        },
        "id": "-_1HaqhZ2UWC",
        "outputId": "d0073a8c-a93e-425a-c27c-672b925e586c"
      },
      "outputs": [],
      "source": [
        "def get_cos_coeffs(model):\n",
        "    logits = model(dataset)[:, -1]\n",
        "    logits = einops.rearrange(logits, \"(a b) c -> a b c\", a=p, b=p)\n",
        "    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])\n",
        "    return vals\n",
        "\n",
        "\n",
        "get_metrics(model, metric_cache, get_cos_coeffs, \"cos_coeffs\")\n",
        "print(metric_cache[\"cos_coeffs\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx80vCX-2UWC",
        "outputId": "909273d0-9d4f-4026-f70b-24f7846b9b3d"
      },
      "outputs": [],
      "source": [
        "fig = line(metric_cache[\"cos_coeffs\"].T, line_labels=[f\"Freq {i}\" for i in range(1, p//2+1)], title=\"Coefficients with Predicted Logits\", xaxis=\"Epoch\", x=checkpoint_epochs, yaxis=\"Coefficient\", return_fig=True)\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "64c4305b8e8f466f8dd9240817fbc5e2"
          ]
        },
        "id": "M_vIfqHw2UWC",
        "outputId": "2c1b11ba-51f3-40d1-d4ea-acc7e4ab2906"
      },
      "outputs": [],
      "source": [
        "def get_cos_sim(model):\n",
        "    logits = model(dataset)[:, -1]\n",
        "    logits = einops.rearrange(logits, \"(a b) c -> a b c\", a=p, b=p)\n",
        "    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])\n",
        "    return vals / logits.norm()\n",
        "\n",
        "get_metrics(model, metric_cache, get_cos_sim, \"cos_sim\") # You may need a big GPU. If you don't have one and can't work around this, raise an issue for help!\n",
        "print(metric_cache[\"cos_sim\"].shape)\n",
        "\n",
        "fig = line(metric_cache[\"cos_sim\"].T, line_labels=[f\"Freq {i}\" for i in range(1, p//2+1)], title=\"Cosine Sim with Predicted Logits\", xaxis=\"Epoch\", x=checkpoint_epochs, yaxis=\"Cosine Sim\", return_fig=True)\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "93e129d45acf43d29dc422409440ad70"
          ]
        },
        "id": "gdNT2ys-2UWD",
        "outputId": "a86c212b-582f-484a-e834-741108bb3782"
      },
      "outputs": [],
      "source": [
        "def get_residual_cos_sim(model):\n",
        "    logits = model(dataset)[:, -1]\n",
        "    logits = einops.rearrange(logits, \"(a b) c -> a b c\", a=p, b=p)\n",
        "    vals = (cos_cube * logits[None, :, :, :]).sum([-3, -2, -1])\n",
        "    residual = logits - (vals[:, None, None, None] * cos_cube).sum(dim=0)\n",
        "    return residual.norm() / logits.norm()\n",
        "\n",
        "get_metrics(model, metric_cache, get_residual_cos_sim, \"residual_cos_sim\")\n",
        "print(metric_cache[\"residual_cos_sim\"].shape)\n",
        "\n",
        "fig = line([metric_cache[\"cos_sim\"][:, i] for i in range(p//2)]+[metric_cache[\"residual_cos_sim\"]], line_labels=[f\"Freq {i}\" for i in range(1, p//2+1)]+[\"residual\"], title=\"Cosine Sim with Predicted Logits + Residual\", xaxis=\"Epoch\", x=checkpoint_epochs, yaxis=\"Cosine Sim\", return_fig=True)\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSwcrGN42UWD"
      },
      "source": [
        "## Restricted Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ll4ANw62UWD",
        "outputId": "eb247de4-274b-4de4-b2a7-6e5b26ca184a"
      },
      "outputs": [],
      "source": [
        "neuron_acts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb7SzJxC2UWD",
        "outputId": "bc81f91c-53f2-4108-eb18-66be1d716e7a"
      },
      "outputs": [],
      "source": [
        "neuron_acts_square = einops.rearrange(neuron_acts, \"(a b) neur -> a b neur\", a=p, b=p).clone()\n",
        "# Center it\n",
        "neuron_acts_square -= einops.reduce(neuron_acts_square, \"a b neur -> 1 1 neur\", \"mean\")\n",
        "neuron_acts_square_fourier = einsum(\"a b neur, fa a, fb b -> fa fb neur\", neuron_acts_square, fourier_basis, fourier_basis)\n",
        "imshow(neuron_acts_square_fourier.norm(dim=-1), xaxis=\"Fourier Component b\", yaxis=\"Fourier Component a\", title=\"Norms of neuron activations by Fourier Component\", x=fourier_basis_names, y=fourier_basis_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baKaHHHe2UWD",
        "outputId": "e94a0f6a-e395-4fec-e0e2-7cc676dc7a01"
      },
      "outputs": [],
      "source": [
        "original_logits, cache = model.run_with_cache(dataset)\n",
        "print(original_logits.numel())\n",
        "neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T47cdrQJ2UWD",
        "outputId": "9ea8ae5a-1e90-4340-cc5f-4e460a62dea5"
      },
      "outputs": [],
      "source": [
        "approx_neuron_acts = torch.zeros_like(neuron_acts)\n",
        "approx_neuron_acts += neuron_acts.mean(dim=0)\n",
        "a = torch.arange(p)[:, None]\n",
        "b = torch.arange(p)[None, :]\n",
        "for freq in key_freqs:\n",
        "    cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "    cos_apb_vec /= cos_apb_vec.norm()\n",
        "    cos_apb_vec = einops.rearrange(cos_apb_vec, \"a b -> (a b) 1\")\n",
        "    approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec\n",
        "    sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "    sin_apb_vec /= sin_apb_vec.norm()\n",
        "    sin_apb_vec = einops.rearrange(sin_apb_vec, \"a b -> (a b) 1\")\n",
        "    approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec\n",
        "restricted_logits = approx_neuron_acts @ W_logit\n",
        "print(loss_fn(restricted_logits[test_indices], test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9utdvBE2UWD",
        "outputId": "ddfdb172-eac4-4df7-f6e5-e8ef02119ab4"
      },
      "outputs": [],
      "source": [
        "print(loss_fn(all_logits, labels)) # This bugged on models not fully trained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy72wtlN2UWD"
      },
      "source": [
        "### Look During Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvX1sp7d2UWD",
        "outputId": "56039b47-6493-42ce-b6fd-9d55808933d7"
      },
      "outputs": [],
      "source": [
        "def get_restricted_loss(model):\n",
        "    logits, cache = model.run_with_cache(dataset)\n",
        "    logits = logits[:, -1, :]\n",
        "    neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]\n",
        "    approx_neuron_acts = torch.zeros_like(neuron_acts)\n",
        "    approx_neuron_acts += neuron_acts.mean(dim=0)\n",
        "    a = torch.arange(p)[:, None]\n",
        "    b = torch.arange(p)[None, :]\n",
        "    for freq in key_freqs:\n",
        "        cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "        cos_apb_vec /= cos_apb_vec.norm()\n",
        "        cos_apb_vec = einops.rearrange(cos_apb_vec, \"a b -> (a b) 1\")\n",
        "        approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec\n",
        "        sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "        sin_apb_vec /= sin_apb_vec.norm()\n",
        "        sin_apb_vec = einops.rearrange(sin_apb_vec, \"a b -> (a b) 1\")\n",
        "        approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec\n",
        "    restricted_logits = approx_neuron_acts @ model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
        "    # Add bias term\n",
        "    restricted_logits += logits.mean(dim=0, keepdim=True) - restricted_logits.mean(dim=0, keepdim=True)\n",
        "    return loss_fn(restricted_logits[test_indices], test_labels)\n",
        "get_restricted_loss(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3079070c7f5445aabbc6a198a53b04b2"
          ]
        },
        "id": "ve3FweXG2UWD",
        "outputId": "790a3cf2-c88d-4e3e-cbbf-c0316ed1ac48"
      },
      "outputs": [],
      "source": [
        "get_metrics(model, metric_cache, get_restricted_loss, \"restricted_loss\", reset=True)\n",
        "print(metric_cache[\"restricted_loss\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHIwg6zO2UWD",
        "outputId": "da0b4330-e1f4-42fc-8698-d8e6a45902f2"
      },
      "outputs": [],
      "source": [
        "fig = line([train_losses[::100], test_losses[::100], metric_cache[\"restricted_loss\"]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Restricted Loss Curve\", line_labels=['train', 'test', \"restricted_loss\"], toggle_x=True, toggle_y=True, return_fig=True)\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFh5Lkvy2UWD",
        "outputId": "15ea8635-3f4f-4aee-e5e4-257f63732f45"
      },
      "outputs": [],
      "source": [
        "fig = line([torch.tensor(test_losses[::100])/metric_cache[\"restricted_loss\"]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Restricted Loss to Test Loss Ratio\", toggle_x=True, toggle_y=True, return_fig=True)\n",
        "# WARNING: bugged when cancelling training half way thr ough\n",
        "add_lines(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1On6MAO02UWD"
      },
      "source": [
        "## Excluded Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsDR1s5x2UWD",
        "outputId": "3a733bc0-1ca9-4f25-c22e-40f6e4e539d7"
      },
      "outputs": [],
      "source": [
        "approx_neuron_acts = torch.zeros_like(neuron_acts)\n",
        "# approx_neuron_acts += neuron_acts.mean(dim=0)\n",
        "a = torch.arange(p)[:, None]\n",
        "b = torch.arange(p)[None, :]\n",
        "for freq in key_freqs:\n",
        "    cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "    cos_apb_vec /= cos_apb_vec.norm()\n",
        "    cos_apb_vec = einops.rearrange(cos_apb_vec, \"a b -> (a b) 1\")\n",
        "    approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec\n",
        "    sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "    sin_apb_vec /= sin_apb_vec.norm()\n",
        "    sin_apb_vec = einops.rearrange(sin_apb_vec, \"a b -> (a b) 1\")\n",
        "    approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec\n",
        "excluded_neuron_acts = neuron_acts - approx_neuron_acts\n",
        "excluded_logits = excluded_neuron_acts @ W_logit\n",
        "print(loss_fn(excluded_logits[train_indices], train_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLK6WdFs2UWD",
        "outputId": "bd6a5f20-6234-4bb1-cfbb-82a0081e2c93"
      },
      "outputs": [],
      "source": [
        "def get_excluded_loss(model):\n",
        "    logits, cache = model.run_with_cache(dataset)\n",
        "    logits = logits[:, -1, :]\n",
        "    neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]\n",
        "    approx_neuron_acts = torch.zeros_like(neuron_acts)\n",
        "    # approx_neuron_acts += neuron_acts.mean(dim=0)\n",
        "    a = torch.arange(p)[:, None]\n",
        "    b = torch.arange(p)[None, :]\n",
        "    for freq in key_freqs:\n",
        "        cos_apb_vec = torch.cos(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "        cos_apb_vec /= cos_apb_vec.norm()\n",
        "        cos_apb_vec = einops.rearrange(cos_apb_vec, \"a b -> (a b) 1\")\n",
        "        approx_neuron_acts += (neuron_acts * cos_apb_vec).sum(dim=0) * cos_apb_vec\n",
        "        sin_apb_vec = torch.sin(freq * 2 * torch.pi / p * (a + b)).to(device)\n",
        "        sin_apb_vec /= sin_apb_vec.norm()\n",
        "        sin_apb_vec = einops.rearrange(sin_apb_vec, \"a b -> (a b) 1\")\n",
        "        approx_neuron_acts += (neuron_acts * sin_apb_vec).sum(dim=0) * sin_apb_vec\n",
        "    excluded_neuron_acts = neuron_acts - approx_neuron_acts\n",
        "    residual_stream_final = excluded_neuron_acts @ model.blocks[0].mlp.W_out + cache[\"resid_mid\", 0][:, -1, :]\n",
        "    excluded_logits = residual_stream_final @ model.unembed.W_U\n",
        "    return loss_fn(excluded_logits[train_indices], train_labels)\n",
        "get_excluded_loss(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3661a3612a0744aa806d8f7c2e884666"
          ]
        },
        "id": "HFG8fslR2UWD",
        "outputId": "e04f9b0a-469b-4016-e078-dfdc62b66a14"
      },
      "outputs": [],
      "source": [
        "get_metrics(model, metric_cache, get_excluded_loss, \"excluded_loss\", reset=True)\n",
        "print(metric_cache[\"excluded_loss\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqEDrPYn2UWD",
        "outputId": "7687fbfc-d9f3-4e72-9b67-95c199a7d121"
      },
      "outputs": [],
      "source": [
        "fig = line([train_losses[::100], test_losses[::100], metric_cache[\"excluded_loss\"], metric_cache[\"restricted_loss\"]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Excluded and Restricted Loss Curve\", line_labels=['train', 'test', \"excluded_loss\", \"restricted_loss\"], toggle_x=True, toggle_y=True, return_fig=True)\n",
        "\n",
        "add_lines(fig)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8859a5491331dba93123a91c2831400aced845b502848170e05fcb48b2c144be"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
